{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Welcome to Modal notebooks!\n",
                "\n",
                "Write Python code and collaborate in real time. Your code runs in Modal's\n",
                "**serverless cloud**, and anyone in the same workspace can join.\n",
                "\n",
                "This notebook comes with some common Python libraries installed. Run\n",
                "cells with `Shift+Enter`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CUDA (NVIDIA GPU) is not available.\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(\"CUDA (NVIDIA GPU) is available.\")\n",
                "    print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n",
                "    for i in range(torch.cuda.device_count()):\n",
                "        print(f\"  Device {i} Name: {torch.cuda.get_device_name(i)}\")\n",
                "else:\n",
                "    print(\"CUDA (NVIDIA GPU) is not available.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created or found directory: ./GenImage\n",
                        "Current directory: /root/GenImage\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "target_dir = './GenImage'\n",
                "os.makedirs(target_dir, exist_ok=True)\n",
                "print(f\"Created or found directory: {target_dir}\")\n",
                "os.chdir(target_dir)\n",
                "print(f\"Current directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "406df655",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['WeatherDiffusion']\n"
                    ]
                }
            ],
            "source": [
                "print(os.listdir())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "2d18cbef",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Repo already exists at WeatherDiffusion, skipping clone.\n",
                        "Now in repo: /root/GenImage/WeatherDiffusion\n",
                        "Git remotes:\n",
                        "origin\thttps://github.com/Hadayxinchao/WeatherDiffusion.git (fetch)\n",
                        "origin\thttps://github.com/Hadayxinchao/WeatherDiffusion.git (push)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import subprocess, os, sys\n",
                "\n",
                "repo_url_ssh = \"https://github.com/Hadayxinchao/WeatherDiffusion.git\"\n",
                "repo_url_https = \"https://github.com/Hadayxinchao/WeatherDiffusion.git\"\n",
                "repo_dir = \"WeatherDiffusion\"\n",
                "\n",
                "if os.path.exists(repo_dir):\n",
                "    print(f\"Repo already exists at {repo_dir}, skipping clone.\")\n",
                "else:\n",
                "    try:\n",
                "        print(f\"Cloning via SSH: {repo_url_ssh}\")\n",
                "        subprocess.check_call([\"git\", \"clone\", repo_url_ssh])\n",
                "    except Exception as e:\n",
                "        print(f\"SSH clone failed ({e}); falling back to HTTPS...\")\n",
                "        subprocess.check_call([\"git\", \"clone\", repo_url_https])\n",
                "\n",
                "os.chdir(repo_dir)\n",
                "print(\"Now in repo:\", os.getcwd())\n",
                "print(\"Git remotes:\")\n",
                "subprocess.check_call([\"git\", \"remote\", \"-v\"])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "f44d6abb",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['.git', '.github', '.gitignore', 'LICENSE', 'README.md', 'calculate_psnr_ssim.py', 'configs', 'datasets', 'eval_diffusion.py', 'models', 'test_ohaze.py', 'train_diffusion.py', 'unet_finetune.ipynb', 'utils', 'weatherdiff.ipynb', 'data']\n"
                    ]
                }
            ],
            "source": [
                "print(os.listdir())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0317c1fa",
            "metadata": {},
            "source": [
                "# Data prepare"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "300f7536",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\r\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
                        "‚¨áÔ∏è ƒêang t·∫£i file zip (ID: 1tPXAPoyQVHwriAfkEcOn7Q9yop4Z7ev4)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Downloading...\n",
                        "From (original): https://drive.google.com/uc?id=1tPXAPoyQVHwriAfkEcOn7Q9yop4Z7ev4\n",
                        "From (redirected): https://drive.google.com/uc?id=1tPXAPoyQVHwriAfkEcOn7Q9yop4Z7ev4&confirm=t&uuid=b195faeb-8efd-4bde-8c91-72485235d48b\n",
                        "To: /root/GenImage/WeatherDiffusion/dataset.zip\n",
                        "\r  0%|                                                                               | 0.00/377M [00:00<?, ?B/s]\r  1%|‚ñç                                                                     | 2.62M/377M [00:00<00:39, 9.44MB/s]\r  2%|‚ñà‚ñé                                                                    | 6.82M/377M [00:00<00:28, 13.1MB/s]\r  4%|‚ñà‚ñà‚ñä                                                                   | 15.2M/377M [00:00<00:12, 29.8MB/s]\r  6%|‚ñà‚ñà‚ñà‚ñà‚ñç                                                                 | 23.6M/377M [00:00<00:08, 39.5MB/s]\r  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                | 29.9M/377M [00:00<00:07, 45.2MB/s]\r 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                               | 38.3M/377M [00:01<00:07, 45.6MB/s]\r 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                             | 44.6M/377M [00:01<00:08, 41.4MB/s]\r 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                           | 57.1M/377M [00:01<00:06, 47.0MB/s]\r 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                          | 64.0M/377M [00:01<00:06, 51.1MB/s]\r 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                       | 77.6M/377M [00:01<00:05, 57.0MB/s]\r 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 83.9M/377M [00:01<00:05, 56.5MB/s]\r 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                    | 95.9M/377M [00:01<00:03, 70.5MB/s]\r 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                   | 104M/377M [00:02<00:04, 60.0MB/s]\r 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                 | 113M/377M [00:02<00:03, 66.4MB/s]\r 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                | 122M/377M [00:02<00:04, 55.6MB/s]\r 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                             | 137M/377M [00:02<00:03, 76.6MB/s]\r 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                           | 147M/377M [00:02<00:04, 54.7MB/s]\r 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 157M/377M [00:03<00:03, 63.7MB/s]\r 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                       | 167M/377M [00:03<00:03, 63.3MB/s]\r 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                      | 175M/377M [00:03<00:03, 56.8MB/s]\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                   | 188M/377M [00:03<00:02, 71.1MB/s]\r 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                  | 197M/377M [00:03<00:03, 51.7MB/s]\r 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                              | 213M/377M [00:03<00:02, 72.7MB/s]\r 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                             | 223M/377M [00:04<00:02, 59.8MB/s]\r 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                           | 232M/377M [00:04<00:02, 61.9MB/s]\r 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                         | 242M/377M [00:04<00:02, 56.0MB/s]\r 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 252M/377M [00:04<00:02, 60.2MB/s]\r 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 267M/377M [00:04<00:01, 65.5MB/s]\r 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 275M/377M [00:04<00:01, 68.5MB/s]\r 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                | 290M/377M [00:04<00:01, 85.2MB/s]\r 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 300M/377M [00:05<00:01, 74.9MB/s]\r 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 309M/377M [00:05<00:01, 66.4MB/s]\r 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 316M/377M [00:05<00:00, 63.0MB/s]\r 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 326M/377M [00:05<00:00, 68.0MB/s]\r 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 333M/377M [00:05<00:00, 69.2MB/s]\r 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä     | 349M/377M [00:05<00:00, 89.7MB/s]\r 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 359M/377M [00:06<00:00, 73.7MB/s]\r 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 368M/377M [00:06<00:00, 69.3MB/s]\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 377M/377M [00:06<00:00, 60.7MB/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì¶ ƒêang gi·∫£i n√©n d·ªØ li·ªáu...\n",
                        "‚úÖ ƒê√£ gi·∫£i n√©n th√†nh c√¥ng v√†o: ./data/custom_haze\n",
                        "üìä T·ªïng s·ªë file ·∫£nh t√¨m th·∫•y: 5482\n"
                    ]
                }
            ],
            "source": [
                "# Install gdown for downloading from Google Drive\n",
                "!pip install -q gdown\n",
                "\n",
                "import gdown\n",
                "import zipfile\n",
                "import os\n",
                "import shutil\n",
                "\n",
                "# --- C·∫§U H√åNH ---\n",
                "FILE_ID = '1tPXAPoyQVHwriAfkEcOn7Q9yop4Z7ev4' \n",
                "\n",
                "# 2. ƒê∆∞·ªùng d·∫´n t·∫£i v·ªÅ v√† th∆∞ m·ª•c ƒë√≠ch\n",
                "DOWNLOAD_OUTPUT = 'dataset.zip'\n",
                "EXTRACT_DIR = './data/custom_haze' # Th∆∞ m·ª•c ch·ª©a ·∫£nh sau khi gi·∫£i n√©n\n",
                "\n",
                "# --- X·ª¨ L√ù ---\n",
                "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
                "if not os.path.exists(EXTRACT_DIR):\n",
                "    os.makedirs(EXTRACT_DIR)\n",
                "\n",
                "# Download file ZIP\n",
                "print(f\"‚¨áÔ∏è ƒêang t·∫£i file zip (ID: {FILE_ID})...\")\n",
                "try:\n",
                "    # D√πng fuzzy=True ƒë·ªÉ gdown t·ª± tr√≠ch xu·∫•t ID n·∫øu b·∫°n l·ª° paste c·∫£ link\n",
                "    gdown.download(id=FILE_ID, output=DOWNLOAD_OUTPUT, quiet=False, fuzzy=True)\n",
                "    \n",
                "    # Ki·ªÉm tra file t·∫£i v·ªÅ\n",
                "    if os.path.exists(DOWNLOAD_OUTPUT):\n",
                "        print(\"üì¶ ƒêang gi·∫£i n√©n d·ªØ li·ªáu...\")\n",
                "        try:\n",
                "            with zipfile.ZipFile(DOWNLOAD_OUTPUT, 'r') as zip_ref:\n",
                "                zip_ref.extractall(EXTRACT_DIR)\n",
                "            print(f\"‚úÖ ƒê√£ gi·∫£i n√©n th√†nh c√¥ng v√†o: {EXTRACT_DIR}\")\n",
                "            \n",
                "            # X√≥a file zip cho nh·∫π m√°y\n",
                "            os.remove(DOWNLOAD_OUTPUT)\n",
                "            \n",
                "            # Ki·ªÉm tra nhanh s·ªë l∆∞·ª£ng file\n",
                "            num_files = sum([len(files) for r, d, files in os.walk(EXTRACT_DIR)])\n",
                "            print(f\"üìä T·ªïng s·ªë file ·∫£nh t√¨m th·∫•y: {num_files}\")\n",
                "            \n",
                "        except zipfile.BadZipFile:\n",
                "            print(\"‚ùå L·ªói: File t·∫£i v·ªÅ kh√¥ng ph·∫£i l√† file zip h·ª£p l·ªá. H√£y ki·ªÉm tra l·∫°i link Drive.\")\n",
                "    else:\n",
                "        print(\"‚ùå L·ªói: Kh√¥ng t·∫£i ƒë∆∞·ª£c file. Ki·ªÉm tra l·∫°i ID v√† quy·ªÅn truy c·∫≠p (Share: Anyone with link).\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå C√≥ l·ªói x·∫£y ra: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "0424c4d9",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking structure for: ./data/custom_haze\n",
                        "Dataset structure:\n",
                        "custom_haze/\n",
                        "  train.txt\n",
                        "  test.txt\n",
                        "  output/\n",
                        "    test/\n",
                        "      results_prediction/\n",
                        "        161.jpg\n",
                        "        170.jpg\n",
                        "        173.jpg\n",
                        "        176.jpg\n",
                        "        153.jpg\n",
                        "        ... and 445 more files\n",
                        "      test_haze/\n",
                        "        161.jpg\n",
                        "        170.jpg\n",
                        "        176.jpg\n",
                        "        164.jpg\n",
                        "        151.jpg\n",
                        "        ... and 445 more files\n",
                        "      test_origin/\n",
                        "        96.jpg\n",
                        "        99.jpg\n",
                        "        82.jpg\n",
                        "        95.jpg\n",
                        "        88.jpg\n",
                        "        ... and 445 more files\n",
                        "    train/\n",
                        "      train_haze/\n",
                        "        1612.jpg\n",
                        "        1582.jpg\n",
                        "        1733.jpg\n",
                        "        1714.jpg\n",
                        "        1619.jpg\n",
                        "        ... and 2060 more files\n",
                        "      train_origin/\n",
                        "        994.jpg\n",
                        "        986.jpg\n",
                        "        998.jpg\n",
                        "        985.jpg\n",
                        "        990.jpg\n",
                        "        ... and 2060 more files\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "# ƒê·ªãnh nghƒ©a l·∫°i bi·∫øn data_dir cho kh·ªõp v·ªõi th∆∞ m·ª•c gi·∫£i n√©n\n",
                "data_dir = './data/custom_haze' \n",
                "\n",
                "# Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c\n",
                "if os.path.exists(data_dir):\n",
                "    print(f\"Checking structure for: {data_dir}\")\n",
                "    print(\"Dataset structure:\")\n",
                "    for root, dirs, files in os.walk(data_dir):\n",
                "        level = root.replace(data_dir, '').count(os.sep)\n",
                "        indent = ' ' * 2 * level\n",
                "        print(f'{indent}{os.path.basename(root)}/')\n",
                "        subindent = ' ' * 2 * (level + 1)\n",
                "        for file in files[:5]:  # Only show first 5 files\n",
                "            print(f'{subindent}{file}')\n",
                "        if len(files) > 5:\n",
                "            print(f'{subindent}... and {len(files) - 5} more files')\n",
                "else:\n",
                "    print(f\"‚ùå Th∆∞ m·ª•c {data_dir} ch∆∞a t·ªìn t·∫°i. H√£y ch·∫°y cell t·∫£i d·ªØ li·ªáu (Data prepare) ·ªü tr√™n tr∆∞·ªõc!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "7673e011",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output directory: ./data/custom_haze/output\n",
                        "Contents: ['test', 'train']\n"
                    ]
                }
            ],
            "source": [
                "# Di chuy·ªÉn v√†o th∆∞ m·ª•c output (n∆°i ch·ª©a train v√† test folders)\n",
                "output_dir = os.path.join(data_dir, 'output')\n",
                "print(f\"Output directory: {output_dir}\")\n",
                "print(f\"Contents: {os.listdir(output_dir) if os.path.exists(output_dir) else 'Directory not found'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1af50efc",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 2065 training haze images\n",
                        "Found 450 test haze images\n",
                        "Found 450 test origin images\n",
                        "Created ./data/custom_haze/train.txt\n",
                        "Created ./data/custom_haze/test.txt\n",
                        "\n",
                        "First 3 lines of train.txt:\n",
                        "  ./data/custom_haze/output/train/train_haze/1000.jpg ./data/custom_haze/output/train/train_haze/1000.jpg\n",
                        "  ./data/custom_haze/output/train/train_haze/1001.jpg ./data/custom_haze/output/train/train_haze/1001.jpg\n",
                        "  ./data/custom_haze/output/train/train_haze/1002.jpg ./data/custom_haze/output/train/train_haze/1002.jpg\n",
                        "\n",
                        "First 3 lines of test.txt:\n",
                        "  ./data/custom_haze/output/test/test_haze/10.jpg ./data/custom_haze/output/test/test_origin/10.jpg\n",
                        "  ./data/custom_haze/output/test/test_haze/100.jpg ./data/custom_haze/output/test/test_origin/100.jpg\n",
                        "  ./data/custom_haze/output/test/test_haze/102.jpg ./data/custom_haze/output/test/test_origin/102.jpg\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "# T·∫°o file danh s√°ch cho training set\n",
                "# V·ªõi custom dataset, train_haze ch·ª©a ·∫£nh haze nh∆∞ng kh√¥ng c√≥ GT ri√™ng\n",
                "# Ch√∫ng ta c·∫ßn t·∫°o paired list gi·ªØa haze v√† origin images\n",
                "\n",
                "train_haze_dir = os.path.join(output_dir, 'train', 'train_haze')\n",
                "test_haze_dir = os.path.join(output_dir, 'test', 'test_haze')\n",
                "test_origin_dir = os.path.join(output_dir, 'test', 'test_origin')\n",
                "\n",
                "# Get all training haze images\n",
                "train_haze_images = sorted(glob.glob(os.path.join(train_haze_dir, '*.*')))\n",
                "print(f\"Found {len(train_haze_images)} training haze images\")\n",
                "\n",
                "# Get all test images\n",
                "test_haze_images = sorted(glob.glob(os.path.join(test_haze_dir, '*.*')))\n",
                "test_origin_images = sorted(glob.glob(os.path.join(test_origin_dir, '*.*')))\n",
                "print(f\"Found {len(test_haze_images)} test haze images\")\n",
                "print(f\"Found {len(test_origin_images)} test origin images\")\n",
                "\n",
                "# Create train.txt - for self-supervised training (haze image as both input and target)\n",
                "# Or you can use haze as input and haze as output for denoising\n",
                "train_list_path = os.path.join(data_dir, 'train.txt')\n",
                "with open(train_list_path, 'w') as f:\n",
                "    for haze_img in train_haze_images:\n",
                "        # Format: input_path gt_path\n",
                "        # N·∫øu kh√¥ng c√≥ GT ri√™ng, d√πng ch√≠nh ·∫£nh haze l√†m target (self-supervised)\n",
                "        # Ho·∫∑c b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh n·∫øu c√≥ GT ri√™ng\n",
                "        f.write(f\"{haze_img} {haze_img}\\n\")\n",
                "print(f\"Created {train_list_path}\")\n",
                "\n",
                "# Create test.txt - paired haze and origin images\n",
                "test_list_path = os.path.join(data_dir, 'test.txt')\n",
                "with open(test_list_path, 'w') as f:\n",
                "    for haze_img, origin_img in zip(test_haze_images, test_origin_images):\n",
                "        f.write(f\"{haze_img} {origin_img}\\n\")\n",
                "print(f\"Created {test_list_path}\")\n",
                "\n",
                "# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n\n",
                "print(\"\\nFirst 3 lines of train.txt:\")\n",
                "with open(train_list_path, 'r') as f:\n",
                "    for i, line in enumerate(f):\n",
                "        if i >= 3:\n",
                "            break\n",
                "        print(f\"  {line.strip()}\")\n",
                "\n",
                "print(\"\\nFirst 3 lines of test.txt:\")\n",
                "with open(test_list_path, 'r') as f:\n",
                "    for i, line in enumerate(f):\n",
                "        if i >= 3:\n",
                "            break\n",
                "        print(f\"  {line.strip()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "247befc2",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "30565ef6",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Current directory: /root/GenImage/WeatherDiffusion\n",
                        "\n",
                        "Data directory structure:\n",
                        "data_dir in config: /root/GenImage/WeatherDiffusion/data/custom_haze\n",
                        "Exists: True\n",
                        "\n",
                        "train.txt exists: True\n",
                        "test.txt exists: True\n"
                    ]
                }
            ],
            "source": [
                "# Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c cu·ªëi c√πng\n",
                "print(\"Current directory:\", os.getcwd())\n",
                "print(\"\\nData directory structure:\")\n",
                "print(f\"data_dir in config: {os.path.join(os.getcwd(), 'data', 'custom_haze')}\")\n",
                "print(f\"Exists: {os.path.exists(os.path.join(os.getcwd(), 'data', 'custom_haze'))}\")\n",
                "\n",
                "# Ki·ªÉm tra file lists\n",
                "train_txt = os.path.join(os.getcwd(), 'data', 'custom_haze', 'train.txt')\n",
                "test_txt = os.path.join(os.getcwd(), 'data', 'custom_haze', 'test.txt')\n",
                "print(f\"\\ntrain.txt exists: {os.path.exists(train_txt)}\")\n",
                "print(f\"test.txt exists: {os.path.exists(test_txt)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f866ca4",
            "metadata": {},
            "source": [
                "# Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6434bb2f",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "=> using dataset 'MyDataset'\n",
                        "Data Loaded!\n",
                        "=> creating denoising-diffusion model...\n",
                        "Found 40 images in ./haze_data/train/hazy\n",
                        "Found 5 images in ./haze_data/test/hazy\n",
                        "Adam (\n",
                        "Parameter Group 0\n",
                        "    amsgrad: False\n",
                        "    betas: (0.9, 0.999)\n",
                        "    capturable: False\n",
                        "    decoupled_weight_decay: False\n",
                        "    differentiable: False\n",
                        "    eps: 1e-08\n",
                        "    foreach: None\n",
                        "    fused: None\n",
                        "    lr: 1e-05\n",
                        "    maximize: False\n",
                        "    weight_decay: 0.001\n",
                        ")\n",
                        "Current learning rate before training: 1e-05\n",
                        "epoch:  0\n",
                        "epoch:  1\n",
                        "epoch:  2\n",
                        "epoch:  3\n",
                        "step: 10, loss: 10449.59375, data time: 3.966639518737793\n",
                        "epoch:  4\n",
                        "epoch:  5\n",
                        "epoch:  6\n",
                        "step: 20, loss: 7910.8818359375, data time: 2.0040950775146484\n",
                        "epoch:  7\n",
                        "epoch:  8\n",
                        "epoch:  9\n",
                        "step: 30, loss: 6066.8623046875, data time: 1.216320514678955\n",
                        "epoch:  10\n",
                        "epoch:  11\n",
                        "epoch:  12\n",
                        "epoch:  13\n",
                        "step: 40, loss: 4578.3701171875, data time: 3.9093542098999023\n",
                        "epoch:  14\n",
                        "epoch:  15\n",
                        "epoch:  16\n",
                        "step: 50, loss: 3654.35595703125, data time: 1.8904286623001099\n",
                        "epoch:  17\n",
                        "epoch:  18\n",
                        "epoch:  19\n",
                        "step: 60, loss: 2999.480712890625, data time: 1.2505474090576172\n",
                        "epoch:  20\n",
                        "epoch:  21\n",
                        "epoch:  22\n",
                        "epoch:  23\n",
                        "step: 70, loss: 2328.123291015625, data time: 3.6077487468719482\n",
                        "epoch:  24\n",
                        "epoch:  25\n",
                        "epoch:  26\n",
                        "step: 80, loss: 1998.734375, data time: 2.0665682554244995\n",
                        "epoch:  27\n",
                        "epoch:  28\n",
                        "epoch:  29\n",
                        "step: 90, loss: 1601.3033447265625, data time: 1.3202279408772786\n",
                        "epoch:  30\n",
                        "epoch:  31\n",
                        "epoch:  32\n",
                        "epoch:  33\n",
                        "step: 100, loss: 1619.08154296875, data time: 3.9670186042785645\n",
                        "epoch:  34\n",
                        "epoch:  35\n",
                        "epoch:  36\n",
                        "step: 110, loss: 1751.0496826171875, data time: 2.0298469066619873\n",
                        "epoch:  37\n",
                        "epoch:  38\n",
                        "epoch:  39\n",
                        "step: 120, loss: 1560.27587890625, data time: 1.3946480751037598\n",
                        "epoch:  40\n",
                        "epoch:  41\n",
                        "epoch:  42\n",
                        "epoch:  43\n",
                        "step: 130, loss: 1151.0296630859375, data time: 3.960113048553467\n",
                        "epoch:  44\n",
                        "epoch:  45\n",
                        "epoch:  46\n",
                        "step: 140, loss: 1089.331298828125, data time: 1.946029782295227\n",
                        "epoch:  47\n",
                        "epoch:  48\n",
                        "epoch:  49\n",
                        "step: 150, loss: 1182.261962890625, data time: 1.8288023471832275\n",
                        "epoch:  50\n",
                        "epoch:  51\n",
                        "epoch:  52\n",
                        "epoch:  53\n",
                        "step: 160, loss: 1228.560791015625, data time: 3.4779250621795654\n",
                        "epoch:  54\n",
                        "epoch:  55\n",
                        "epoch:  56\n",
                        "step: 170, loss: 1094.6834716796875, data time: 1.9688591957092285\n",
                        "epoch:  57\n",
                        "epoch:  58\n",
                        "epoch:  59\n",
                        "step: 180, loss: 909.6630249023438, data time: 1.3415783246358235\n",
                        "epoch:  60\n",
                        "epoch:  61\n",
                        "epoch:  62\n",
                        "epoch:  63\n",
                        "step: 190, loss: 1020.5654296875, data time: 7.3150718212127686\n",
                        "epoch:  64\n",
                        "epoch:  65\n",
                        "epoch:  66\n",
                        "step: 200, loss: 899.057861328125, data time: 1.911590814590454\n",
                        "epoch:  67\n",
                        "epoch:  68\n",
                        "epoch:  69\n",
                        "step: 210, loss: 957.312744140625, data time: 1.1105680465698242\n",
                        "epoch:  70\n",
                        "epoch:  71\n",
                        "epoch:  72\n",
                        "epoch:  73\n",
                        "step: 220, loss: 863.3402709960938, data time: 3.9849443435668945\n",
                        "epoch:  74\n",
                        "epoch:  75\n",
                        "epoch:  76\n",
                        "step: 230, loss: 913.9273681640625, data time: 2.0259562730789185\n",
                        "epoch:  77\n",
                        "epoch:  78\n",
                        "epoch:  79\n",
                        "step: 240, loss: 776.6533203125, data time: 1.1597603956858318\n",
                        "epoch:  80\n",
                        "epoch:  81\n",
                        "epoch:  82\n",
                        "epoch:  83\n",
                        "step: 250, loss: 851.2958984375, data time: 3.7224884033203125\n",
                        "epoch:  84\n",
                        "epoch:  85\n",
                        "epoch:  86\n",
                        "step: 260, loss: 697.6805419921875, data time: 1.9817721843719482\n",
                        "epoch:  87\n",
                        "epoch:  88\n",
                        "epoch:  89\n",
                        "step: 270, loss: 788.108154296875, data time: 1.279380162556966\n",
                        "epoch:  90\n",
                        "epoch:  91\n",
                        "epoch:  92\n",
                        "epoch:  93\n",
                        "step: 280, loss: 811.453369140625, data time: 3.8484437465667725\n",
                        "epoch:  94\n",
                        "epoch:  95\n",
                        "epoch:  96\n",
                        "step: 290, loss: 619.5855712890625, data time: 1.861177921295166\n",
                        "epoch:  97\n",
                        "epoch:  98\n",
                        "epoch:  99\n",
                        "step: 300, loss: 871.5001220703125, data time: 1.4382130304972331\n",
                        "epoch:  100\n",
                        "epoch:  101\n",
                        "epoch:  102\n",
                        "epoch:  103\n",
                        "step: 310, loss: 762.0074462890625, data time: 4.101428508758545\n",
                        "epoch:  104\n",
                        "epoch:  105\n",
                        "epoch:  106\n",
                        "step: 320, loss: 681.5320434570312, data time: 1.8085548877716064\n",
                        "epoch:  107\n",
                        "epoch:  108\n",
                        "epoch:  109\n",
                        "step: 330, loss: 694.752685546875, data time: 1.3324302832285564\n",
                        "epoch:  110\n",
                        "epoch:  111\n",
                        "epoch:  112\n",
                        "epoch:  113\n",
                        "step: 340, loss: 751.605224609375, data time: 3.8509597778320312\n",
                        "epoch:  114\n",
                        "epoch:  115\n",
                        "epoch:  116\n",
                        "step: 350, loss: 709.281005859375, data time: 1.9939839839935303\n",
                        "epoch:  117\n",
                        "epoch:  118\n",
                        "epoch:  119\n",
                        "step: 360, loss: 669.2396240234375, data time: 1.253727118174235\n",
                        "epoch:  120\n",
                        "epoch:  121\n",
                        "epoch:  122\n",
                        "epoch:  123\n",
                        "step: 370, loss: 683.6912841796875, data time: 3.850118637084961\n",
                        "epoch:  124\n",
                        "epoch:  125\n",
                        "epoch:  126\n",
                        "step: 380, loss: 639.7000732421875, data time: 2.1335989236831665\n",
                        "epoch:  127\n",
                        "epoch:  128\n",
                        "epoch:  129\n",
                        "step: 390, loss: 598.0828857421875, data time: 1.200808842976888\n",
                        "epoch:  130\n",
                        "epoch:  131\n",
                        "epoch:  132\n",
                        "epoch:  133\n",
                        "step: 400, loss: 512.5703125, data time: 4.000312805175781\n",
                        "epoch:  134\n",
                        "epoch:  135\n",
                        "epoch:  136\n",
                        "step: 410, loss: 563.02001953125, data time: 1.8854901790618896\n",
                        "epoch:  137\n",
                        "epoch:  138\n",
                        "epoch:  139\n",
                        "step: 420, loss: 837.8837280273438, data time: 1.2856493790944417\n",
                        "epoch:  140\n",
                        "epoch:  141\n",
                        "epoch:  142\n",
                        "epoch:  143\n",
                        "step: 430, loss: 508.0115661621094, data time: 3.956552028656006\n",
                        "epoch:  144\n",
                        "epoch:  145\n",
                        "epoch:  146\n",
                        "step: 440, loss: 597.4619140625, data time: 1.9221352338790894\n",
                        "epoch:  147\n",
                        "epoch:  148\n",
                        "epoch:  149\n",
                        "step: 450, loss: 545.5528564453125, data time: 1.3025484879811604\n",
                        "epoch:  150\n",
                        "epoch:  151\n",
                        "epoch:  152\n",
                        "epoch:  153\n",
                        "step: 460, loss: 578.3157348632812, data time: 3.8280999660491943\n",
                        "epoch:  154\n",
                        "epoch:  155\n",
                        "epoch:  156\n",
                        "step: 470, loss: 612.5364990234375, data time: 2.1831270456314087\n",
                        "epoch:  157\n",
                        "epoch:  158\n",
                        "epoch:  159\n",
                        "step: 480, loss: 539.8193359375, data time: 1.2441234588623047\n",
                        "epoch:  160\n",
                        "epoch:  161\n",
                        "epoch:  162\n",
                        "epoch:  163\n",
                        "step: 490, loss: 584.7849731445312, data time: 3.936171770095825\n",
                        "epoch:  164\n",
                        "epoch:  165\n",
                        "epoch:  166\n",
                        "step: 500, loss: 533.1047973632812, data time: 2.07497775554657\n",
                        "epoch:  167\n",
                        "epoch:  168\n",
                        "epoch:  169\n",
                        "step: 510, loss: 444.148681640625, data time: 1.4765104452768962\n",
                        "epoch:  170\n",
                        "epoch:  171\n",
                        "epoch:  172\n",
                        "epoch:  173\n",
                        "step: 520, loss: 468.10662841796875, data time: 3.922029495239258\n",
                        "epoch:  174\n",
                        "epoch:  175\n",
                        "epoch:  176\n",
                        "step: 530, loss: 509.1757507324219, data time: 1.8524980545043945\n",
                        "epoch:  177\n",
                        "epoch:  178\n",
                        "epoch:  179\n",
                        "step: 540, loss: 417.01116943359375, data time: 1.4577248096466064\n",
                        "epoch:  180\n",
                        "epoch:  181\n",
                        "epoch:  182\n",
                        "epoch:  183\n",
                        "step: 550, loss: 458.3440246582031, data time: 3.964219808578491\n",
                        "epoch:  184\n",
                        "epoch:  185\n",
                        "epoch:  186\n",
                        "step: 560, loss: 457.5442199707031, data time: 2.127004861831665\n",
                        "epoch:  187\n",
                        "epoch:  188\n",
                        "epoch:  189\n",
                        "step: 570, loss: 424.67718505859375, data time: 1.3711412747701008\n",
                        "epoch:  190\n",
                        "epoch:  191\n",
                        "epoch:  192\n",
                        "epoch:  193\n",
                        "step: 580, loss: 464.15582275390625, data time: 3.9893858432769775\n",
                        "epoch:  194\n",
                        "epoch:  195\n",
                        "epoch:  196\n",
                        "step: 590, loss: 445.82659912109375, data time: 2.0872533321380615\n",
                        "epoch:  197\n",
                        "epoch:  198\n",
                        "epoch:  199\n",
                        "step: 600, loss: 513.0509033203125, data time: 1.330126444498698\n",
                        "epoch:  200\n",
                        "epoch:  201\n",
                        "epoch:  202\n",
                        "epoch:  203\n",
                        "step: 610, loss: 371.4843444824219, data time: 3.879063844680786\n",
                        "epoch:  204\n",
                        "epoch:  205\n",
                        "epoch:  206\n",
                        "step: 620, loss: 431.45330810546875, data time: 2.006491184234619\n",
                        "epoch:  207\n",
                        "epoch:  208\n",
                        "epoch:  209\n",
                        "step: 630, loss: 373.06298828125, data time: 1.2102793057759602\n",
                        "epoch:  210\n",
                        "epoch:  211\n",
                        "epoch:  212\n",
                        "epoch:  213\n",
                        "step: 640, loss: 417.97021484375, data time: 4.094879865646362\n",
                        "epoch:  214\n",
                        "epoch:  215\n",
                        "epoch:  216\n",
                        "step: 650, loss: 456.09954833984375, data time: 1.978270411491394\n",
                        "epoch:  217\n",
                        "epoch:  218\n",
                        "epoch:  219\n",
                        "step: 660, loss: 479.9539794921875, data time: 1.322482983271281\n",
                        "epoch:  220\n",
                        "epoch:  221\n",
                        "epoch:  222\n",
                        "epoch:  223\n",
                        "step: 670, loss: 485.2142028808594, data time: 4.122793197631836\n",
                        "epoch:  224\n",
                        "epoch:  225\n",
                        "epoch:  226\n",
                        "step: 680, loss: 316.25604248046875, data time: 1.806857943534851\n",
                        "epoch:  227\n",
                        "epoch:  228\n",
                        "epoch:  229\n",
                        "step: 690, loss: 479.54547119140625, data time: 1.3504010836283367\n",
                        "epoch:  230\n",
                        "epoch:  231\n",
                        "epoch:  232\n",
                        "epoch:  233\n",
                        "step: 700, loss: 390.129638671875, data time: 3.966474771499634\n",
                        "epoch:  234\n",
                        "epoch:  235\n",
                        "epoch:  236\n",
                        "step: 710, loss: 316.1207275390625, data time: 1.9527990818023682\n",
                        "epoch:  237\n",
                        "epoch:  238\n",
                        "epoch:  239\n",
                        "step: 720, loss: 473.33612060546875, data time: 1.3983516693115234\n",
                        "epoch:  240\n",
                        "epoch:  241\n",
                        "epoch:  242\n",
                        "epoch:  243\n",
                        "step: 730, loss: 389.3334655761719, data time: 4.5170512199401855\n",
                        "epoch:  244\n",
                        "epoch:  245\n",
                        "epoch:  246\n",
                        "step: 740, loss: 314.1866760253906, data time: 1.8864902257919312\n",
                        "epoch:  247\n",
                        "epoch:  248\n",
                        "epoch:  249\n",
                        "step: 750, loss: 415.04913330078125, data time: 1.3287355105082195\n",
                        "epoch:  250\n",
                        "epoch:  251\n",
                        "epoch:  252\n",
                        "epoch:  253\n",
                        "step: 760, loss: 309.15625, data time: 3.9135518074035645\n",
                        "epoch:  254\n",
                        "epoch:  255\n",
                        "epoch:  256\n",
                        "step: 770, loss: 332.90423583984375, data time: 2.0475434064865112\n",
                        "epoch:  257\n",
                        "epoch:  258\n",
                        "epoch:  259\n",
                        "step: 780, loss: 370.42669677734375, data time: 1.2848798433939617\n",
                        "epoch:  260\n",
                        "epoch:  261\n",
                        "epoch:  262\n",
                        "epoch:  263\n",
                        "step: 790, loss: 398.10089111328125, data time: 3.662125587463379\n",
                        "epoch:  264\n",
                        "epoch:  265\n",
                        "epoch:  266\n",
                        "step: 800, loss: 416.8569030761719, data time: 1.8910905122756958\n",
                        "epoch:  267\n",
                        "epoch:  268\n",
                        "epoch:  269\n",
                        "step: 810, loss: 317.634033203125, data time: 1.4075308640797932\n",
                        "epoch:  270\n",
                        "epoch:  271\n",
                        "epoch:  272\n",
                        "epoch:  273\n",
                        "step: 820, loss: 410.4156494140625, data time: 3.7517013549804688\n",
                        "epoch:  274\n",
                        "epoch:  275\n",
                        "epoch:  276\n",
                        "step: 830, loss: 344.1376037597656, data time: 1.9855618476867676\n",
                        "epoch:  277\n",
                        "epoch:  278\n",
                        "epoch:  279\n",
                        "step: 840, loss: 268.0711364746094, data time: 1.2606216271718342\n",
                        "epoch:  280\n",
                        "epoch:  281\n",
                        "epoch:  282\n",
                        "epoch:  283\n",
                        "step: 850, loss: 313.84979248046875, data time: 3.787360906600952\n",
                        "epoch:  284\n",
                        "epoch:  285\n",
                        "epoch:  286\n",
                        "step: 860, loss: 300.0908203125, data time: 1.9383548498153687\n",
                        "epoch:  287\n",
                        "epoch:  288\n",
                        "epoch:  289\n",
                        "step: 870, loss: 340.275634765625, data time: 1.4585660298665364\n",
                        "epoch:  290\n",
                        "epoch:  291\n",
                        "epoch:  292\n",
                        "epoch:  293\n",
                        "step: 880, loss: 304.263916015625, data time: 4.7299065589904785\n",
                        "epoch:  294\n",
                        "epoch:  295\n",
                        "epoch:  296\n",
                        "step: 890, loss: 403.0545349121094, data time: 1.9303950071334839\n",
                        "epoch:  297\n",
                        "epoch:  298\n",
                        "epoch:  299\n",
                        "step: 900, loss: 313.7989501953125, data time: 1.349972089131673\n",
                        "epoch:  300\n",
                        "epoch:  301\n",
                        "epoch:  302\n",
                        "epoch:  303\n",
                        "step: 910, loss: 242.47265625, data time: 4.0571489334106445\n",
                        "epoch:  304\n",
                        "epoch:  305\n",
                        "epoch:  306\n",
                        "step: 920, loss: 280.8134765625, data time: 1.7323458194732666\n",
                        "epoch:  307\n",
                        "epoch:  308\n",
                        "epoch:  309\n",
                        "step: 930, loss: 283.82318115234375, data time: 1.3602397441864014\n",
                        "epoch:  310\n",
                        "epoch:  311\n",
                        "epoch:  312\n",
                        "epoch:  313\n",
                        "step: 940, loss: 402.0337219238281, data time: 3.9860217571258545\n",
                        "epoch:  314\n",
                        "epoch:  315\n",
                        "epoch:  316\n",
                        "step: 950, loss: 343.1173095703125, data time: 2.071505904197693\n",
                        "epoch:  317\n",
                        "epoch:  318\n",
                        "epoch:  319\n",
                        "step: 960, loss: 257.325439453125, data time: 1.2728466192881267\n",
                        "epoch:  320\n",
                        "epoch:  321\n",
                        "epoch:  322\n",
                        "epoch:  323\n",
                        "step: 970, loss: 282.66180419921875, data time: 3.8612494468688965\n",
                        "epoch:  324\n",
                        "epoch:  325\n",
                        "epoch:  326\n",
                        "step: 980, loss: 316.4095764160156, data time: 1.9926189184188843\n",
                        "epoch:  327\n",
                        "epoch:  328\n",
                        "epoch:  329\n",
                        "step: 990, loss: 267.32965087890625, data time: 1.422168493270874\n",
                        "epoch:  330\n",
                        "epoch:  331\n",
                        "epoch:  332\n",
                        "epoch:  333\n",
                        "step: 1000, loss: 318.9363098144531, data time: 3.9121806621551514\n",
                        "epoch:  334\n",
                        "epoch:  335\n",
                        "epoch:  336\n",
                        "step: 1010, loss: 289.10858154296875, data time: 1.91279935836792\n",
                        "epoch:  337\n",
                        "epoch:  338\n",
                        "epoch:  339\n",
                        "step: 1020, loss: 275.3155822753906, data time: 1.2384325663248699\n",
                        "epoch:  340\n",
                        "epoch:  341\n",
                        "epoch:  342\n",
                        "epoch:  343\n",
                        "step: 1030, loss: 262.2912902832031, data time: 3.806276321411133\n",
                        "epoch:  344\n",
                        "epoch:  345\n",
                        "epoch:  346\n",
                        "step: 1040, loss: 277.1558532714844, data time: 2.093623161315918\n",
                        "epoch:  347\n",
                        "epoch:  348\n",
                        "epoch:  349\n",
                        "step: 1050, loss: 215.6900177001953, data time: 1.2928649584452312\n",
                        "epoch:  350\n",
                        "epoch:  351\n",
                        "epoch:  352\n",
                        "epoch:  353\n",
                        "step: 1060, loss: 296.4847412109375, data time: 3.896120309829712\n",
                        "epoch:  354\n",
                        "epoch:  355\n",
                        "epoch:  356\n",
                        "step: 1070, loss: 264.2783203125, data time: 2.1123169660568237\n",
                        "epoch:  357\n",
                        "epoch:  358\n",
                        "epoch:  359\n",
                        "step: 1080, loss: 495.5527648925781, data time: 1.4759453137715657\n",
                        "epoch:  360\n",
                        "epoch:  361\n",
                        "epoch:  362\n",
                        "epoch:  363\n",
                        "step: 1090, loss: 339.10076904296875, data time: 4.074446439743042\n",
                        "epoch:  364\n",
                        "epoch:  365\n",
                        "epoch:  366\n",
                        "step: 1100, loss: 272.42926025390625, data time: 1.8611242771148682\n",
                        "epoch:  367\n",
                        "epoch:  368\n",
                        "epoch:  369\n",
                        "step: 1110, loss: 214.14085388183594, data time: 1.3246352672576904\n",
                        "epoch:  370\n",
                        "epoch:  371\n",
                        "epoch:  372\n",
                        "epoch:  373\n",
                        "step: 1120, loss: 262.64788818359375, data time: 4.435516119003296\n",
                        "epoch:  374\n",
                        "epoch:  375\n",
                        "epoch:  376\n",
                        "step: 1130, loss: 291.2638854980469, data time: 2.0001566410064697\n",
                        "epoch:  377\n",
                        "epoch:  378\n",
                        "epoch:  379\n",
                        "step: 1140, loss: 213.98365783691406, data time: 1.3812889258066814\n",
                        "epoch:  380\n",
                        "epoch:  381\n",
                        "epoch:  382\n",
                        "epoch:  383\n",
                        "step: 1150, loss: 318.4077453613281, data time: 4.037122488021851\n",
                        "epoch:  384\n",
                        "epoch:  385\n",
                        "epoch:  386\n",
                        "step: 1160, loss: 239.82289123535156, data time: 2.010972499847412\n",
                        "epoch:  387\n",
                        "epoch:  388\n",
                        "epoch:  389\n",
                        "step: 1170, loss: 304.2602844238281, data time: 1.2218246459960938\n",
                        "epoch:  390\n",
                        "epoch:  391\n",
                        "epoch:  392\n",
                        "epoch:  393\n",
                        "step: 1180, loss: 237.0679931640625, data time: 3.7806522846221924\n",
                        "epoch:  394\n",
                        "epoch:  395\n",
                        "epoch:  396\n",
                        "step: 1190, loss: 205.41888427734375, data time: 1.9838842153549194\n",
                        "epoch:  397\n",
                        "epoch:  398\n",
                        "epoch:  399\n",
                        "step: 1200, loss: 213.94845581054688, data time: 1.3047748406728108\n",
                        "epoch:  400\n",
                        "epoch:  401\n",
                        "epoch:  402\n",
                        "epoch:  403\n",
                        "step: 1210, loss: 238.90054321289062, data time: 3.5992724895477295\n",
                        "epoch:  404\n",
                        "epoch:  405\n",
                        "epoch:  406\n",
                        "step: 1220, loss: 290.06787109375, data time: 2.031987190246582\n",
                        "epoch:  407\n",
                        "epoch:  408\n",
                        "epoch:  409\n",
                        "step: 1230, loss: 240.68716430664062, data time: 1.3425292174021404\n",
                        "epoch:  410\n",
                        "epoch:  411\n",
                        "epoch:  412\n",
                        "epoch:  413\n",
                        "step: 1240, loss: 185.50909423828125, data time: 3.9974005222320557\n",
                        "epoch:  414\n",
                        "epoch:  415\n",
                        "epoch:  416\n",
                        "step: 1250, loss: 309.2707214355469, data time: 2.34239661693573\n",
                        "epoch:  417\n",
                        "epoch:  418\n",
                        "epoch:  419\n",
                        "step: 1260, loss: 295.7041320800781, data time: 1.3865984280904133\n",
                        "epoch:  420\n",
                        "epoch:  421\n",
                        "epoch:  422\n",
                        "epoch:  423\n",
                        "step: 1270, loss: 262.3793640136719, data time: 4.176132917404175\n",
                        "epoch:  424\n",
                        "epoch:  425\n",
                        "epoch:  426\n",
                        "step: 1280, loss: 202.47976684570312, data time: 1.9345133304595947\n",
                        "epoch:  427\n",
                        "epoch:  428\n",
                        "epoch:  429\n",
                        "step: 1290, loss: 214.83335876464844, data time: 1.309187412261963\n",
                        "epoch:  430\n",
                        "epoch:  431\n",
                        "epoch:  432\n",
                        "epoch:  433\n",
                        "step: 1300, loss: 249.9780731201172, data time: 3.6510984897613525\n",
                        "epoch:  434\n",
                        "epoch:  435\n",
                        "epoch:  436\n",
                        "step: 1310, loss: 254.3648681640625, data time: 1.9265525341033936\n",
                        "epoch:  437\n",
                        "epoch:  438\n",
                        "epoch:  439\n",
                        "step: 1320, loss: 250.9263916015625, data time: 1.2882194519042969\n",
                        "epoch:  440\n",
                        "epoch:  441\n",
                        "epoch:  442\n",
                        "epoch:  443\n",
                        "step: 1330, loss: 215.5574188232422, data time: 4.15283465385437\n",
                        "epoch:  444\n",
                        "epoch:  445\n",
                        "epoch:  446\n",
                        "step: 1340, loss: 258.23876953125, data time: 1.7778226137161255\n",
                        "epoch:  447\n",
                        "epoch:  448\n",
                        "epoch:  449\n",
                        "step: 1350, loss: 297.26983642578125, data time: 1.3844739596048992\n",
                        "epoch:  450\n",
                        "epoch:  451\n",
                        "epoch:  452\n",
                        "epoch:  453\n",
                        "step: 1360, loss: 229.1116943359375, data time: 3.7566514015197754\n",
                        "epoch:  454\n",
                        "epoch:  455\n",
                        "epoch:  456\n",
                        "step: 1370, loss: 215.83969116210938, data time: 2.0823986530303955\n",
                        "epoch:  457\n",
                        "epoch:  458\n",
                        "epoch:  459\n",
                        "step: 1380, loss: 242.93032836914062, data time: 1.3331151008605957\n",
                        "epoch:  460\n",
                        "epoch:  461\n",
                        "epoch:  462\n",
                        "epoch:  463\n",
                        "step: 1390, loss: 279.536376953125, data time: 3.6538493633270264\n",
                        "epoch:  464\n",
                        "epoch:  465\n",
                        "epoch:  466\n",
                        "step: 1400, loss: 236.8448486328125, data time: 2.1104565858840942\n",
                        "epoch:  467\n",
                        "epoch:  468\n",
                        "epoch:  469\n",
                        "step: 1410, loss: 318.5130615234375, data time: 1.375980536142985\n",
                        "epoch:  470\n",
                        "epoch:  471\n",
                        "epoch:  472\n",
                        "epoch:  473\n",
                        "step: 1420, loss: 290.2738037109375, data time: 4.101664066314697\n",
                        "epoch:  474\n",
                        "epoch:  475\n",
                        "epoch:  476\n",
                        "step: 1430, loss: 236.1016845703125, data time: 1.9658281803131104\n",
                        "epoch:  477\n",
                        "epoch:  478\n",
                        "epoch:  479\n",
                        "step: 1440, loss: 210.63351440429688, data time: 1.252926270167033\n",
                        "epoch:  480\n",
                        "epoch:  481\n",
                        "epoch:  482\n",
                        "epoch:  483\n",
                        "step: 1450, loss: 271.9696044921875, data time: 3.67012619972229\n",
                        "epoch:  484\n",
                        "epoch:  485\n",
                        "epoch:  486\n",
                        "step: 1460, loss: 210.42388916015625, data time: 1.9466490745544434\n",
                        "epoch:  487\n",
                        "epoch:  488\n",
                        "epoch:  489\n",
                        "step: 1470, loss: 276.60467529296875, data time: 1.3286949793497722\n",
                        "epoch:  490\n",
                        "epoch:  491\n",
                        "epoch:  492\n",
                        "epoch:  493\n",
                        "step: 1480, loss: 268.9566650390625, data time: 4.227530241012573\n",
                        "epoch:  494\n",
                        "epoch:  495\n",
                        "epoch:  496\n",
                        "step: 1490, loss: 225.604248046875, data time: 2.0219916105270386\n",
                        "epoch:  497\n",
                        "epoch:  498\n",
                        "epoch:  499\n",
                        "step: 1500, loss: 218.32815551757812, data time: 1.2044291496276855\n",
                        "epoch:  500\n",
                        "epoch:  501\n",
                        "epoch:  502\n",
                        "epoch:  503\n",
                        "step: 1510, loss: 202.67745971679688, data time: 4.087566137313843\n",
                        "epoch:  504\n",
                        "epoch:  505\n",
                        "epoch:  506\n",
                        "step: 1520, loss: 238.4493408203125, data time: 2.0092703104019165\n",
                        "epoch:  507\n",
                        "epoch:  508\n",
                        "epoch:  509\n",
                        "step: 1530, loss: 226.71754455566406, data time: 1.3955822785695393\n",
                        "epoch:  510\n",
                        "epoch:  511\n",
                        "epoch:  512\n",
                        "epoch:  513\n",
                        "step: 1540, loss: 230.70704650878906, data time: 4.21475076675415\n",
                        "epoch:  514\n",
                        "epoch:  515\n",
                        "epoch:  516\n",
                        "step: 1550, loss: 265.37481689453125, data time: 1.952627182006836\n",
                        "epoch:  517\n",
                        "epoch:  518\n",
                        "epoch:  519\n",
                        "step: 1560, loss: 195.03744506835938, data time: 1.369486888249715\n",
                        "epoch:  520\n",
                        "epoch:  521\n",
                        "epoch:  522\n",
                        "epoch:  523\n",
                        "step: 1570, loss: 260.52447509765625, data time: 3.5956203937530518\n",
                        "epoch:  524\n",
                        "epoch:  525\n",
                        "epoch:  526\n",
                        "step: 1580, loss: 210.22531127929688, data time: 2.1834362745285034\n",
                        "epoch:  527\n",
                        "epoch:  528\n",
                        "epoch:  529\n",
                        "step: 1590, loss: 160.78857421875, data time: 1.4937747319539387\n",
                        "epoch:  530\n",
                        "epoch:  531\n",
                        "epoch:  532\n",
                        "epoch:  533\n",
                        "step: 1600, loss: 228.374755859375, data time: 3.7727227210998535\n",
                        "epoch:  534\n",
                        "epoch:  535\n",
                        "epoch:  536\n",
                        "step: 1610, loss: 311.606689453125, data time: 1.7882919311523438\n",
                        "epoch:  537\n",
                        "epoch:  538\n",
                        "epoch:  539\n",
                        "step: 1620, loss: 290.6451721191406, data time: 1.399947722752889\n",
                        "epoch:  540\n",
                        "epoch:  541\n",
                        "epoch:  542\n",
                        "epoch:  543\n",
                        "step: 1630, loss: 199.3939208984375, data time: 4.281156778335571\n",
                        "epoch:  544\n",
                        "epoch:  545\n",
                        "epoch:  546\n",
                        "step: 1640, loss: 222.89407348632812, data time: 1.8666805028915405\n",
                        "epoch:  547\n",
                        "epoch:  548\n",
                        "epoch:  549\n",
                        "step: 1650, loss: 328.2221984863281, data time: 1.3758631547292073\n",
                        "epoch:  550\n",
                        "epoch:  551\n",
                        "epoch:  552\n",
                        "epoch:  553\n",
                        "step: 1660, loss: 210.06076049804688, data time: 4.175677061080933\n",
                        "epoch:  554\n",
                        "epoch:  555\n",
                        "epoch:  556\n",
                        "step: 1670, loss: 201.42001342773438, data time: 1.896755337715149\n",
                        "epoch:  557\n",
                        "epoch:  558\n",
                        "epoch:  559\n",
                        "step: 1680, loss: 150.17123413085938, data time: 1.3358176549275715\n",
                        "epoch:  560\n",
                        "epoch:  561\n",
                        "epoch:  562\n",
                        "epoch:  563\n",
                        "epoch:  563\n",
                        "step: 1690, loss: 218.7119140625, data time: 3.945373773574829\n",
                        "epoch:  564\n",
                        "epoch:  565\n",
                        "epoch:  566\n",
                        "step: 1700, loss: 272.4873352050781, data time: 1.9055720567703247\n",
                        "epoch:  567\n",
                        "epoch:  568\n",
                        "epoch:  569\n",
                        "step: 1710, loss: 177.7628631591797, data time: 1.1521498362223308\n",
                        "epoch:  570\n",
                        "epoch:  571\n",
                        "epoch:  572\n",
                        "epoch:  573\n",
                        "step: 1720, loss: 205.5540008544922, data time: 3.6243844032287598\n",
                        "epoch:  574\n",
                        "epoch:  575\n",
                        "epoch:  576\n",
                        "step: 1730, loss: 181.33447265625, data time: 2.156827688217163\n",
                        "epoch:  577\n",
                        "epoch:  578\n",
                        "epoch:  579\n",
                        "step: 1740, loss: 301.7763366699219, data time: 1.353122393290202\n",
                        "epoch:  580\n",
                        "epoch:  581\n",
                        "epoch:  582\n",
                        "epoch:  583\n",
                        "step: 1750, loss: 163.86550903320312, data time: 4.350924491882324\n",
                        "epoch:  584\n",
                        "epoch:  585\n",
                        "epoch:  586\n",
                        "step: 1760, loss: 175.42404174804688, data time: 2.05990207195282\n",
                        "epoch:  587\n",
                        "epoch:  588\n",
                        "epoch:  589\n",
                        "step: 1770, loss: 195.73703002929688, data time: 1.336267630259196\n",
                        "epoch:  590\n",
                        "epoch:  591\n",
                        "epoch:  592\n",
                        "epoch:  593\n",
                        "step: 1780, loss: 245.49977111816406, data time: 4.0515217781066895\n",
                        "epoch:  594\n",
                        "epoch:  595\n",
                        "epoch:  596\n",
                        "step: 1790, loss: 167.71255493164062, data time: 2.070825695991516\n",
                        "epoch:  597\n",
                        "epoch:  598\n",
                        "epoch:  599\n",
                        "step: 1800, loss: 250.94808959960938, data time: 1.2381751537322998\n",
                        "epoch:  600\n",
                        "epoch:  601\n",
                        "epoch:  602\n",
                        "epoch:  603\n",
                        "step: 1810, loss: 219.52999877929688, data time: 3.824570655822754\n",
                        "epoch:  604\n",
                        "epoch:  605\n",
                        "epoch:  606\n",
                        "step: 1820, loss: 244.8910369873047, data time: 2.046319007873535\n",
                        "epoch:  607\n",
                        "epoch:  608\n",
                        "epoch:  609\n",
                        "step: 1830, loss: 254.96725463867188, data time: 1.2406679789225261\n",
                        "epoch:  610\n",
                        "epoch:  611\n",
                        "epoch:  612\n",
                        "epoch:  613\n",
                        "step: 1840, loss: 164.33123779296875, data time: 4.412374496459961\n",
                        "epoch:  614\n",
                        "epoch:  615\n",
                        "epoch:  616\n",
                        "step: 1850, loss: 228.7425994873047, data time: 1.9609670639038086\n",
                        "epoch:  617\n",
                        "epoch:  618\n",
                        "epoch:  619\n",
                        "step: 1860, loss: 179.89208984375, data time: 1.2080024083455403\n",
                        "epoch:  620\n",
                        "epoch:  621\n",
                        "epoch:  622\n",
                        "epoch:  623\n",
                        "step: 1870, loss: 218.84014892578125, data time: 3.8308537006378174\n",
                        "epoch:  624\n",
                        "epoch:  625\n",
                        "epoch:  626\n",
                        "step: 1880, loss: 237.80484008789062, data time: 2.036884069442749\n",
                        "epoch:  627\n",
                        "epoch:  628\n",
                        "epoch:  629\n",
                        "step: 1890, loss: 287.688720703125, data time: 1.3472437063852947\n",
                        "epoch:  630\n",
                        "epoch:  631\n",
                        "epoch:  632\n",
                        "epoch:  633\n",
                        "step: 1900, loss: 163.535888671875, data time: 4.395325660705566\n",
                        "epoch:  634\n",
                        "epoch:  635\n",
                        "epoch:  636\n",
                        "step: 1910, loss: 223.18234252929688, data time: 2.096853733062744\n",
                        "epoch:  637\n",
                        "epoch:  638\n",
                        "epoch:  639\n",
                        "step: 1920, loss: 287.4900207519531, data time: 1.4263556798299153\n",
                        "epoch:  640\n",
                        "epoch:  641\n",
                        "epoch:  642\n",
                        "epoch:  643\n",
                        "step: 1930, loss: 171.274658203125, data time: 4.097527980804443\n",
                        "epoch:  644\n",
                        "epoch:  645\n",
                        "epoch:  646\n",
                        "step: 1940, loss: 168.22817993164062, data time: 1.8203896284103394\n",
                        "epoch:  647\n",
                        "epoch:  648\n",
                        "epoch:  649\n",
                        "step: 1950, loss: 220.13796997070312, data time: 1.2526748975118\n",
                        "epoch:  650\n",
                        "epoch:  651\n",
                        "epoch:  652\n",
                        "epoch:  653\n",
                        "step: 1960, loss: 229.31207275390625, data time: 4.013263463973999\n",
                        "epoch:  654\n",
                        "epoch:  655\n",
                        "epoch:  656\n",
                        "step: 1970, loss: 236.72344970703125, data time: 1.8742082118988037\n",
                        "epoch:  657\n",
                        "epoch:  658\n",
                        "epoch:  659\n",
                        "step: 1980, loss: 168.47793579101562, data time: 1.3141605059305828\n",
                        "epoch:  660\n",
                        "epoch:  661\n",
                        "epoch:  662\n",
                        "epoch:  663\n",
                        "step: 1990, loss: 186.78427124023438, data time: 3.926677942276001\n",
                        "epoch:  664\n",
                        "epoch:  665\n",
                        "epoch:  666\n",
                        "step: 2000, loss: 216.84112548828125, data time: 1.9853934049606323\n",
                        "epoch:  667\n",
                        "epoch:  668\n",
                        "epoch:  669\n",
                        "step: 2010, loss: 154.10696411132812, data time: 1.3456393877665203\n",
                        "epoch:  670\n",
                        "epoch:  671\n",
                        "epoch:  672\n",
                        "epoch:  673\n",
                        "step: 2020, loss: 177.04527282714844, data time: 3.6001133918762207\n",
                        "epoch:  674\n",
                        "epoch:  675\n",
                        "epoch:  676\n",
                        "step: 2030, loss: 215.83206176757812, data time: 1.7994471788406372\n",
                        "epoch:  677\n",
                        "epoch:  678\n",
                        "epoch:  679\n",
                        "step: 2040, loss: 274.5765075683594, data time: 1.3058145840962727\n",
                        "epoch:  680\n",
                        "epoch:  681\n",
                        "epoch:  682\n",
                        "epoch:  683\n",
                        "step: 2050, loss: 214.09561157226562, data time: 3.892791271209717\n",
                        "epoch:  684\n",
                        "epoch:  685\n",
                        "epoch:  686\n",
                        "step: 2060, loss: 180.6620635986328, data time: 2.0956664085388184\n",
                        "epoch:  687\n",
                        "epoch:  688\n",
                        "epoch:  689\n",
                        "step: 2070, loss: 271.80194091796875, data time: 1.2026836077372234\n",
                        "epoch:  690\n",
                        "epoch:  691\n",
                        "epoch:  692\n",
                        "epoch:  693\n",
                        "step: 2080, loss: 209.33001708984375, data time: 3.9185633659362793\n",
                        "epoch:  694\n",
                        "epoch:  695\n",
                        "epoch:  696\n",
                        "step: 2090, loss: 302.0989074707031, data time: 2.1434991359710693\n",
                        "epoch:  697\n",
                        "epoch:  698\n",
                        "epoch:  699\n",
                        "step: 2100, loss: 168.02203369140625, data time: 1.3738982677459717\n",
                        "epoch:  700\n",
                        "epoch:  701\n",
                        "epoch:  702\n",
                        "epoch:  703\n",
                        "step: 2110, loss: 166.61007690429688, data time: 4.4385600090026855\n",
                        "epoch:  704\n",
                        "epoch:  705\n",
                        "epoch:  706\n",
                        "step: 2120, loss: 188.27651977539062, data time: 2.174835205078125\n",
                        "epoch:  707\n",
                        "epoch:  708\n",
                        "epoch:  709\n",
                        "step: 2130, loss: 126.81465148925781, data time: 1.2772584756215413\n",
                        "epoch:  710\n",
                        "epoch:  711\n",
                        "epoch:  712\n",
                        "epoch:  713\n",
                        "step: 2140, loss: 140.05679321289062, data time: 3.959888458251953\n",
                        "epoch:  714\n",
                        "epoch:  715\n",
                        "epoch:  716\n",
                        "step: 2150, loss: 213.6725311279297, data time: 1.9824178218841553\n",
                        "epoch:  717\n",
                        "epoch:  718\n",
                        "epoch:  719\n",
                        "step: 2160, loss: 144.88427734375, data time: 1.4180092016855876\n",
                        "epoch:  720\n",
                        "epoch:  721\n",
                        "epoch:  722\n",
                        "epoch:  723\n",
                        "step: 2170, loss: 234.02828979492188, data time: 3.8042242527008057\n",
                        "epoch:  724\n",
                        "epoch:  725\n",
                        "epoch:  726\n",
                        "step: 2180, loss: 190.08901977539062, data time: 2.1397478580474854\n",
                        "epoch:  727\n",
                        "epoch:  728\n",
                        "epoch:  729\n",
                        "step: 2190, loss: 261.3127746582031, data time: 1.268485466639201\n",
                        "epoch:  730\n",
                        "epoch:  731\n",
                        "epoch:  732\n",
                        "epoch:  733\n",
                        "step: 2200, loss: 201.23944091796875, data time: 5.427507638931274\n",
                        "epoch:  734\n",
                        "epoch:  735\n",
                        "epoch:  736\n",
                        "step: 2210, loss: 149.98590087890625, data time: 1.967702865600586\n",
                        "epoch:  737\n",
                        "epoch:  738\n",
                        "epoch:  739\n",
                        "step: 2220, loss: 198.23507690429688, data time: 1.2605055967966716\n",
                        "epoch:  740\n",
                        "epoch:  741\n",
                        "epoch:  742\n",
                        "epoch:  743\n",
                        "step: 2230, loss: 160.96084594726562, data time: 4.382615566253662\n",
                        "epoch:  744\n",
                        "epoch:  745\n",
                        "epoch:  746\n",
                        "step: 2240, loss: 240.099609375, data time: 2.031264305114746\n",
                        "epoch:  747\n",
                        "epoch:  748\n",
                        "epoch:  749\n",
                        "step: 2250, loss: 190.19802856445312, data time: 1.4165574709574382\n",
                        "epoch:  750\n",
                        "epoch:  751\n",
                        "epoch:  752\n",
                        "epoch:  753\n",
                        "step: 2260, loss: 184.5849609375, data time: 4.071346759796143\n",
                        "epoch:  754\n",
                        "epoch:  755\n",
                        "epoch:  756\n",
                        "step: 2270, loss: 123.53421020507812, data time: 1.9582449197769165\n",
                        "epoch:  757\n",
                        "epoch:  758\n",
                        "epoch:  759\n",
                        "step: 2280, loss: 141.6510772705078, data time: 1.4113609790802002\n",
                        "epoch:  760\n",
                        "epoch:  761\n",
                        "epoch:  762\n",
                        "epoch:  763\n",
                        "step: 2290, loss: 162.2639617919922, data time: 4.153703451156616\n",
                        "epoch:  764\n",
                        "epoch:  765\n",
                        "epoch:  766\n",
                        "step: 2300, loss: 187.52114868164062, data time: 1.6441922187805176\n",
                        "epoch:  767\n",
                        "epoch:  768\n",
                        "epoch:  769\n",
                        "step: 2310, loss: 199.390869140625, data time: 1.2742702960968018\n",
                        "epoch:  770\n",
                        "epoch:  771\n",
                        "epoch:  772\n",
                        "epoch:  773\n",
                        "step: 2320, loss: 193.97842407226562, data time: 4.1733739376068115\n",
                        "epoch:  774\n",
                        "epoch:  775\n",
                        "epoch:  776\n",
                        "step: 2330, loss: 203.6727752685547, data time: 2.134101986885071\n",
                        "epoch:  777\n",
                        "epoch:  778\n",
                        "epoch:  779\n",
                        "step: 2340, loss: 151.1910400390625, data time: 1.3690380255381267\n",
                        "epoch:  780\n",
                        "epoch:  781\n",
                        "epoch:  782\n",
                        "epoch:  783\n",
                        "step: 2350, loss: 190.93984985351562, data time: 4.207201242446899\n",
                        "epoch:  784\n",
                        "epoch:  785\n",
                        "epoch:  786\n",
                        "step: 2360, loss: 190.35125732421875, data time: 1.843894362449646\n",
                        "epoch:  787\n",
                        "epoch:  788\n",
                        "epoch:  789\n",
                        "step: 2370, loss: 329.0174560546875, data time: 1.2180242538452148\n",
                        "epoch:  790\n",
                        "epoch:  791\n",
                        "epoch:  792\n",
                        "epoch:  793\n",
                        "step: 2380, loss: 245.98995971679688, data time: 3.939371347427368\n",
                        "epoch:  794\n",
                        "epoch:  795\n",
                        "epoch:  796\n",
                        "step: 2390, loss: 213.65939331054688, data time: 1.9873069524765015\n",
                        "epoch:  797\n",
                        "epoch:  798\n",
                        "epoch:  799\n",
                        "step: 2400, loss: 167.204345703125, data time: 1.3926166693369548\n",
                        "epoch:  800\n",
                        "epoch:  801\n",
                        "epoch:  802\n",
                        "epoch:  803\n",
                        "step: 2410, loss: 231.18153381347656, data time: 4.274468660354614\n",
                        "epoch:  804\n",
                        "epoch:  805\n",
                        "epoch:  806\n",
                        "step: 2420, loss: 173.23910522460938, data time: 2.0841257572174072\n",
                        "epoch:  807\n",
                        "epoch:  808\n",
                        "epoch:  809\n",
                        "step: 2430, loss: 108.67390441894531, data time: 1.348183234532674\n",
                        "epoch:  810\n",
                        "epoch:  811\n",
                        "epoch:  812\n",
                        "epoch:  813\n",
                        "step: 2440, loss: 175.3525390625, data time: 3.728295087814331\n",
                        "epoch:  814\n",
                        "epoch:  815\n",
                        "epoch:  816\n",
                        "step: 2450, loss: 159.47042846679688, data time: 2.0787885189056396\n",
                        "epoch:  817\n",
                        "epoch:  818\n",
                        "epoch:  819\n",
                        "step: 2460, loss: 190.72964477539062, data time: 1.381415605545044\n",
                        "epoch:  820\n",
                        "epoch:  821\n",
                        "epoch:  822\n",
                        "epoch:  823\n",
                        "step: 2470, loss: 182.5775146484375, data time: 3.7612392902374268\n",
                        "epoch:  824\n",
                        "epoch:  825\n",
                        "epoch:  826\n",
                        "step: 2480, loss: 223.31483459472656, data time: 1.9481436014175415\n",
                        "epoch:  827\n",
                        "epoch:  828\n",
                        "epoch:  829\n",
                        "step: 2490, loss: 144.31130981445312, data time: 1.350929896036784\n",
                        "epoch:  830\n",
                        "epoch:  831\n",
                        "epoch:  832\n",
                        "epoch:  833\n",
                        "step: 2500, loss: 240.5491943359375, data time: 3.941213846206665\n",
                        "epoch:  834\n",
                        "epoch:  835\n",
                        "epoch:  836\n",
                        "step: 2510, loss: 245.98959350585938, data time: 1.6929322481155396\n",
                        "epoch:  837\n",
                        "epoch:  838\n",
                        "epoch:  839\n",
                        "step: 2520, loss: 180.54110717773438, data time: 1.3791460990905762\n",
                        "epoch:  840\n",
                        "epoch:  841\n",
                        "epoch:  842\n",
                        "epoch:  843\n",
                        "step: 2530, loss: 200.06143188476562, data time: 3.9257583618164062\n",
                        "epoch:  844\n",
                        "epoch:  845\n",
                        "epoch:  846\n",
                        "step: 2540, loss: 180.9396209716797, data time: 1.9076287746429443\n",
                        "epoch:  847\n",
                        "epoch:  848\n",
                        "epoch:  849\n",
                        "step: 2550, loss: 145.01939392089844, data time: 1.323668082555135\n",
                        "epoch:  850\n",
                        "epoch:  851\n",
                        "epoch:  852\n",
                        "epoch:  853\n",
                        "step: 2560, loss: 267.89959716796875, data time: 3.8038201332092285\n",
                        "epoch:  854\n",
                        "epoch:  855\n",
                        "epoch:  856\n",
                        "step: 2570, loss: 184.1091766357422, data time: 2.0650930404663086\n",
                        "epoch:  857\n",
                        "epoch:  858\n",
                        "epoch:  859\n",
                        "step: 2580, loss: 219.3597412109375, data time: 1.361813227335612\n",
                        "epoch:  860\n",
                        "epoch:  861\n",
                        "epoch:  862\n",
                        "epoch:  863\n",
                        "step: 2590, loss: 234.81910705566406, data time: 3.960972547531128\n",
                        "epoch:  864\n",
                        "epoch:  865\n",
                        "epoch:  866\n",
                        "step: 2600, loss: 160.63079833984375, data time: 1.884137511253357\n",
                        "epoch:  867\n",
                        "epoch:  868\n",
                        "epoch:  869\n",
                        "step: 2610, loss: 170.33419799804688, data time: 1.3176358540852864\n",
                        "epoch:  870\n",
                        "epoch:  871\n",
                        "epoch:  872\n",
                        "epoch:  873\n",
                        "step: 2620, loss: 161.87429809570312, data time: 4.09688401222229\n",
                        "epoch:  874\n",
                        "epoch:  875\n",
                        "epoch:  876\n",
                        "step: 2630, loss: 239.13644409179688, data time: 2.038852334022522\n",
                        "epoch:  877\n",
                        "epoch:  878\n",
                        "epoch:  879\n",
                        "step: 2640, loss: 146.0255889892578, data time: 1.2907636165618896\n",
                        "epoch:  880\n",
                        "epoch:  881\n",
                        "epoch:  882\n",
                        "epoch:  883\n",
                        "step: 2650, loss: 127.38571166992188, data time: 4.252423286437988\n",
                        "epoch:  884\n",
                        "epoch:  885\n",
                        "epoch:  886\n",
                        "step: 2660, loss: 141.8493194580078, data time: 1.9926599264144897\n",
                        "epoch:  887\n",
                        "epoch:  888\n",
                        "epoch:  889\n",
                        "step: 2670, loss: 127.98910522460938, data time: 1.3114535808563232\n",
                        "epoch:  890\n",
                        "epoch:  891\n",
                        "epoch:  892\n",
                        "epoch:  893\n",
                        "step: 2680, loss: 207.577392578125, data time: 3.7510457038879395\n",
                        "epoch:  894\n",
                        "epoch:  895\n",
                        "epoch:  896\n",
                        "step: 2690, loss: 165.31362915039062, data time: 1.814864993095398\n",
                        "epoch:  897\n",
                        "epoch:  898\n",
                        "epoch:  899\n",
                        "step: 2700, loss: 202.3118896484375, data time: 1.4238016605377197\n",
                        "epoch:  900\n",
                        "epoch:  901\n",
                        "epoch:  902\n",
                        "epoch:  903\n",
                        "step: 2710, loss: 163.645751953125, data time: 4.020183801651001\n",
                        "epoch:  904\n",
                        "epoch:  905\n",
                        "epoch:  906\n",
                        "step: 2720, loss: 208.85153198242188, data time: 2.2237229347229004\n",
                        "epoch:  907\n",
                        "epoch:  908\n",
                        "epoch:  909\n",
                        "step: 2730, loss: 168.953125, data time: 1.3197642962137859\n",
                        "epoch:  910\n",
                        "epoch:  911\n",
                        "epoch:  912\n",
                        "epoch:  913\n",
                        "step: 2740, loss: 190.15225219726562, data time: 3.7218198776245117\n",
                        "epoch:  914\n",
                        "epoch:  915\n",
                        "epoch:  916\n",
                        "step: 2750, loss: 149.4175262451172, data time: 2.0072184801101685\n",
                        "epoch:  917\n",
                        "epoch:  918\n",
                        "epoch:  919\n",
                        "step: 2760, loss: 209.50482177734375, data time: 1.2420823574066162\n",
                        "epoch:  920\n",
                        "epoch:  921\n",
                        "epoch:  922\n",
                        "epoch:  923\n",
                        "step: 2770, loss: 171.44223022460938, data time: 3.8819363117218018\n",
                        "epoch:  924\n",
                        "epoch:  925\n",
                        "epoch:  926\n",
                        "step: 2780, loss: 171.66073608398438, data time: 2.171972393989563\n",
                        "epoch:  927\n",
                        "epoch:  928\n",
                        "epoch:  929\n",
                        "step: 2790, loss: 208.5187225341797, data time: 1.3102330366770427\n",
                        "epoch:  930\n",
                        "epoch:  931\n",
                        "epoch:  932\n",
                        "epoch:  933\n",
                        "step: 2800, loss: 136.97677612304688, data time: 3.8371875286102295\n",
                        "epoch:  934\n",
                        "epoch:  935\n",
                        "epoch:  936\n",
                        "step: 2810, loss: 178.66192626953125, data time: 2.053861141204834\n",
                        "epoch:  937\n",
                        "epoch:  938\n",
                        "epoch:  939\n",
                        "step: 2820, loss: 169.96823120117188, data time: 1.3542983531951904\n",
                        "epoch:  940\n",
                        "epoch:  941\n",
                        "epoch:  942\n",
                        "epoch:  943\n",
                        "step: 2830, loss: 166.4651641845703, data time: 4.048894643783569\n",
                        "epoch:  944\n",
                        "epoch:  945\n",
                        "epoch:  946\n",
                        "step: 2840, loss: 206.15023803710938, data time: 2.2354443073272705\n",
                        "epoch:  947\n",
                        "epoch:  948\n",
                        "epoch:  949\n",
                        "step: 2850, loss: 291.1572265625, data time: 1.2543031374613445\n",
                        "epoch:  950\n",
                        "epoch:  951\n",
                        "epoch:  952\n",
                        "epoch:  953\n",
                        "step: 2860, loss: 243.7272491455078, data time: 4.002359867095947\n",
                        "epoch:  954\n",
                        "epoch:  955\n",
                        "epoch:  956\n",
                        "step: 2870, loss: 138.43821716308594, data time: 1.879268765449524\n",
                        "epoch:  957\n",
                        "epoch:  958\n",
                        "epoch:  959\n",
                        "step: 2880, loss: 144.94400024414062, data time: 1.2693689664204915\n",
                        "epoch:  960\n",
                        "epoch:  961\n",
                        "epoch:  962\n",
                        "epoch:  963\n",
                        "step: 2890, loss: 177.28024291992188, data time: 3.786252975463867\n",
                        "epoch:  964\n",
                        "epoch:  965\n",
                        "epoch:  966\n",
                        "step: 2900, loss: 186.99819946289062, data time: 1.9212496280670166\n",
                        "epoch:  967\n",
                        "epoch:  968\n",
                        "epoch:  969\n",
                        "step: 2910, loss: 163.581787109375, data time: 1.2066574891408284\n",
                        "epoch:  970\n",
                        "epoch:  971\n",
                        "epoch:  972\n",
                        "epoch:  973\n",
                        "step: 2920, loss: 163.9713592529297, data time: 3.914909839630127\n",
                        "epoch:  974\n",
                        "epoch:  975\n",
                        "epoch:  976\n",
                        "step: 2930, loss: 170.42922973632812, data time: 1.85811185836792\n",
                        "epoch:  977\n",
                        "epoch:  978\n",
                        "epoch:  979\n",
                        "step: 2940, loss: 241.73016357421875, data time: 1.2436715761820476\n",
                        "epoch:  980\n",
                        "epoch:  981\n",
                        "epoch:  982\n",
                        "epoch:  983\n",
                        "step: 2950, loss: 190.28152465820312, data time: 4.179290294647217\n",
                        "epoch:  984\n",
                        "epoch:  985\n",
                        "epoch:  986\n",
                        "step: 2960, loss: 154.1078643798828, data time: 1.927496075630188\n",
                        "epoch:  987\n",
                        "epoch:  988\n",
                        "epoch:  989\n",
                        "step: 2970, loss: 119.17388916015625, data time: 1.2835421562194824\n",
                        "epoch:  990\n",
                        "epoch:  991\n",
                        "epoch:  992\n",
                        "epoch:  993\n",
                        "step: 2980, loss: 158.98529052734375, data time: 3.998525619506836\n",
                        "epoch:  994\n",
                        "epoch:  995\n",
                        "epoch:  996\n",
                        "step: 2990, loss: 168.08843994140625, data time: 2.078863024711609\n",
                        "epoch:  997\n",
                        "epoch:  998\n",
                        "epoch:  999\n",
                        "step: 3000, loss: 215.01416015625, data time: 1.3247334957122803\n",
                        "epoch:  1000\n",
                        "epoch:  1001\n",
                        "epoch:  1002\n",
                        "epoch:  1003\n",
                        "step: 3010, loss: 212.0859375, data time: 4.292412042617798\n",
                        "epoch:  1004\n",
                        "epoch:  1005\n",
                        "epoch:  1006\n",
                        "step: 3020, loss: 177.05728149414062, data time: 2.1149405241012573\n",
                        "epoch:  1007\n",
                        "epoch:  1008\n",
                        "epoch:  1009\n",
                        "step: 3030, loss: 159.3374786376953, data time: 1.4205489953358967\n",
                        "epoch:  1010\n",
                        "epoch:  1011\n",
                        "epoch:  1012\n",
                        "epoch:  1013\n",
                        "step: 3040, loss: 218.81964111328125, data time: 3.695176601409912\n",
                        "epoch:  1014\n",
                        "epoch:  1015\n",
                        "epoch:  1016\n",
                        "step: 3050, loss: 156.82398986816406, data time: 2.0216535329818726\n",
                        "epoch:  1017\n",
                        "epoch:  1018\n",
                        "epoch:  1019\n",
                        "step: 3060, loss: 153.6294403076172, data time: 1.4126299222310383\n",
                        "epoch:  1020\n",
                        "epoch:  1021\n",
                        "epoch:  1022\n",
                        "epoch:  1023\n",
                        "step: 3070, loss: 151.47854614257812, data time: 4.505248308181763\n",
                        "epoch:  1024\n",
                        "epoch:  1025\n",
                        "epoch:  1026\n",
                        "step: 3080, loss: 222.64613342285156, data time: 1.98304283618927\n",
                        "epoch:  1027\n",
                        "epoch:  1028\n",
                        "epoch:  1029\n",
                        "step: 3090, loss: 233.93960571289062, data time: 1.379538933436076\n",
                        "epoch:  1030\n",
                        "epoch:  1031\n",
                        "epoch:  1032\n",
                        "epoch:  1033\n",
                        "step: 3100, loss: 171.69078063964844, data time: 3.37622332572937\n",
                        "epoch:  1034\n",
                        "epoch:  1035\n",
                        "epoch:  1036\n",
                        "step: 3110, loss: 186.14459228515625, data time: 2.0840632915496826\n",
                        "epoch:  1037\n",
                        "epoch:  1038\n",
                        "epoch:  1039\n",
                        "step: 3120, loss: 156.36444091796875, data time: 1.2057325045267742\n",
                        "epoch:  1040\n",
                        "epoch:  1041\n",
                        "epoch:  1042\n",
                        "epoch:  1043\n",
                        "step: 3130, loss: 173.2274627685547, data time: 4.06346869468689\n",
                        "epoch:  1044\n",
                        "epoch:  1045\n",
                        "epoch:  1046\n",
                        "step: 3140, loss: 223.83090209960938, data time: 1.8004326820373535\n",
                        "epoch:  1047\n",
                        "epoch:  1048\n",
                        "epoch:  1049\n",
                        "step: 3150, loss: 164.7504119873047, data time: 2.0492435296376548\n",
                        "epoch:  1050\n",
                        "epoch:  1051\n",
                        "epoch:  1052\n",
                        "epoch:  1053\n",
                        "step: 3160, loss: 210.9654541015625, data time: 3.8986592292785645\n",
                        "epoch:  1054\n",
                        "epoch:  1055\n",
                        "epoch:  1056\n",
                        "step: 3170, loss: 174.543701171875, data time: 2.0595582723617554\n",
                        "epoch:  1057\n",
                        "epoch:  1058\n",
                        "epoch:  1059\n",
                        "step: 3180, loss: 142.88368225097656, data time: 1.3836545944213867\n",
                        "epoch:  1060\n",
                        "epoch:  1061\n",
                        "epoch:  1062\n",
                        "epoch:  1063\n",
                        "step: 3190, loss: 219.26747131347656, data time: 3.7124087810516357\n",
                        "epoch:  1064\n",
                        "epoch:  1065\n",
                        "epoch:  1066\n",
                        "step: 3200, loss: 147.88113403320312, data time: 1.8707293272018433\n",
                        "epoch:  1067\n",
                        "epoch:  1068\n",
                        "epoch:  1069\n",
                        "step: 3210, loss: 243.29095458984375, data time: 1.2472987174987793\n",
                        "epoch:  1070\n",
                        "epoch:  1071\n",
                        "epoch:  1072\n",
                        "epoch:  1073\n",
                        "step: 3220, loss: 208.67474365234375, data time: 3.9844751358032227\n",
                        "epoch:  1074\n",
                        "epoch:  1075\n",
                        "epoch:  1076\n",
                        "step: 3230, loss: 206.30740356445312, data time: 2.001940369606018\n",
                        "epoch:  1077\n",
                        "epoch:  1078\n",
                        "epoch:  1079\n",
                        "step: 3240, loss: 140.09268188476562, data time: 1.3465043703715007\n",
                        "epoch:  1080\n",
                        "epoch:  1081\n",
                        "epoch:  1082\n",
                        "epoch:  1083\n",
                        "step: 3250, loss: 178.34890747070312, data time: 4.04789924621582\n",
                        "epoch:  1084\n",
                        "epoch:  1085\n",
                        "epoch:  1086\n",
                        "step: 3260, loss: 183.54534912109375, data time: 2.0721882581710815\n",
                        "epoch:  1087\n",
                        "epoch:  1088\n",
                        "epoch:  1089\n",
                        "step: 3270, loss: 175.89492797851562, data time: 1.3110721111297607\n",
                        "epoch:  1090\n",
                        "epoch:  1091\n",
                        "epoch:  1092\n",
                        "epoch:  1093\n",
                        "step: 3280, loss: 182.52151489257812, data time: 3.9843764305114746\n",
                        "epoch:  1094\n",
                        "epoch:  1095\n",
                        "epoch:  1096\n",
                        "step: 3290, loss: 158.9061279296875, data time: 1.974714994430542\n",
                        "epoch:  1097\n",
                        "epoch:  1098\n",
                        "epoch:  1099\n",
                        "step: 3300, loss: 124.25100708007812, data time: 1.3553390502929688\n",
                        "epoch:  1100\n",
                        "epoch:  1101\n",
                        "epoch:  1102\n",
                        "epoch:  1103\n",
                        "step: 3310, loss: 223.24383544921875, data time: 3.7600178718566895\n",
                        "epoch:  1104\n",
                        "epoch:  1105\n",
                        "epoch:  1106\n",
                        "step: 3320, loss: 158.1417236328125, data time: 2.061001181602478\n",
                        "epoch:  1107\n",
                        "epoch:  1108\n",
                        "epoch:  1109\n",
                        "step: 3330, loss: 94.21566009521484, data time: 1.2881604830423992\n",
                        "epoch:  1110\n",
                        "epoch:  1111\n",
                        "epoch:  1112\n",
                        "epoch:  1113\n",
                        "step: 3340, loss: 116.6065444946289, data time: 3.6847140789031982\n",
                        "epoch:  1114\n",
                        "epoch:  1115\n",
                        "epoch:  1116\n",
                        "step: 3350, loss: 160.851806640625, data time: 1.86509370803833\n",
                        "epoch:  1117\n",
                        "epoch:  1118\n",
                        "epoch:  1119\n",
                        "step: 3360, loss: 196.6791534423828, data time: 1.4010738531748455\n",
                        "epoch:  1120\n",
                        "epoch:  1121\n",
                        "epoch:  1122\n",
                        "epoch:  1123\n",
                        "step: 3370, loss: 184.03663635253906, data time: 3.3123042583465576\n",
                        "epoch:  1124\n",
                        "epoch:  1125\n",
                        "epoch:  1126\n",
                        "step: 3380, loss: 159.17364501953125, data time: 1.905862808227539\n",
                        "epoch:  1127\n",
                        "epoch:  1128\n",
                        "epoch:  1129\n",
                        "step: 3390, loss: 228.906982421875, data time: 1.3296527067820232\n",
                        "epoch:  1130\n",
                        "epoch:  1131\n",
                        "epoch:  1132\n",
                        "epoch:  1133\n",
                        "step: 3400, loss: 235.82583618164062, data time: 4.0474326610565186\n",
                        "epoch:  1134\n",
                        "epoch:  1135\n",
                        "epoch:  1136\n",
                        "step: 3410, loss: 165.29562377929688, data time: 1.9522327184677124\n",
                        "epoch:  1137\n",
                        "epoch:  1138\n",
                        "epoch:  1139\n",
                        "step: 3420, loss: 210.06265258789062, data time: 1.2681636015574138\n",
                        "epoch:  1140\n",
                        "epoch:  1141\n",
                        "epoch:  1142\n",
                        "epoch:  1143\n",
                        "step: 3430, loss: 195.2875518798828, data time: 3.9807982444763184\n",
                        "epoch:  1144\n",
                        "epoch:  1145\n",
                        "epoch:  1146\n",
                        "step: 3440, loss: 268.80029296875, data time: 2.167846918106079\n",
                        "epoch:  1147\n",
                        "epoch:  1148\n",
                        "epoch:  1149\n",
                        "step: 3450, loss: 234.58285522460938, data time: 1.1951955954233806\n",
                        "epoch:  1150\n",
                        "epoch:  1151\n",
                        "epoch:  1152\n",
                        "epoch:  1153\n",
                        "step: 3460, loss: 133.59304809570312, data time: 3.83823299407959\n",
                        "epoch:  1154\n",
                        "epoch:  1155\n",
                        "epoch:  1156\n",
                        "step: 3470, loss: 122.67036437988281, data time: 1.8982337713241577\n",
                        "epoch:  1157\n",
                        "epoch:  1158\n",
                        "epoch:  1159\n",
                        "step: 3480, loss: 160.712646484375, data time: 1.2655446529388428\n",
                        "epoch:  1160\n",
                        "epoch:  1161\n",
                        "epoch:  1162\n",
                        "epoch:  1163\n",
                        "step: 3490, loss: 156.97352600097656, data time: 4.05034327507019\n",
                        "epoch:  1164\n",
                        "epoch:  1165\n",
                        "epoch:  1166\n",
                        "step: 3500, loss: 143.859375, data time: 1.9624980688095093\n",
                        "epoch:  1167\n",
                        "epoch:  1168\n",
                        "epoch:  1169\n",
                        "step: 3510, loss: 151.90878295898438, data time: 1.3394804795583088\n",
                        "epoch:  1170\n",
                        "epoch:  1171\n",
                        "epoch:  1172\n",
                        "epoch:  1173\n",
                        "step: 3520, loss: 193.53514099121094, data time: 4.2359068393707275\n",
                        "epoch:  1174\n",
                        "epoch:  1175\n",
                        "epoch:  1176\n",
                        "step: 3530, loss: 177.24624633789062, data time: 1.954262137413025\n",
                        "epoch:  1177\n",
                        "epoch:  1178\n",
                        "epoch:  1179\n",
                        "step: 3540, loss: 139.04397583007812, data time: 1.2674853801727295\n",
                        "epoch:  1180\n",
                        "epoch:  1181\n",
                        "epoch:  1182\n",
                        "epoch:  1183\n",
                        "step: 3550, loss: 235.73626708984375, data time: 3.747627019882202\n",
                        "epoch:  1184\n",
                        "epoch:  1185\n",
                        "epoch:  1186\n",
                        "step: 3560, loss: 142.0647735595703, data time: 1.9333440065383911\n",
                        "epoch:  1187\n",
                        "epoch:  1188\n",
                        "epoch:  1189\n",
                        "step: 3570, loss: 186.0621795654297, data time: 1.2472890218098958\n",
                        "epoch:  1190\n",
                        "epoch:  1191\n",
                        "epoch:  1192\n",
                        "epoch:  1193\n",
                        "step: 3580, loss: 193.0777587890625, data time: 3.967966079711914\n",
                        "epoch:  1194\n",
                        "epoch:  1195\n",
                        "epoch:  1196\n",
                        "step: 3590, loss: 150.32980346679688, data time: 1.9439618587493896\n",
                        "epoch:  1197\n",
                        "epoch:  1198\n",
                        "epoch:  1199\n",
                        "step: 3600, loss: 205.52804565429688, data time: 1.3556443055470784\n",
                        "epoch:  1200\n",
                        "epoch:  1201\n",
                        "epoch:  1202\n",
                        "epoch:  1203\n",
                        "step: 3610, loss: 166.47496032714844, data time: 3.9097752571105957\n",
                        "epoch:  1204\n",
                        "epoch:  1205\n",
                        "epoch:  1206\n",
                        "step: 3620, loss: 107.68199157714844, data time: 1.8907396793365479\n",
                        "epoch:  1207\n",
                        "epoch:  1208\n",
                        "epoch:  1209\n",
                        "step: 3630, loss: 192.164794921875, data time: 1.3257075150807698\n",
                        "epoch:  1210\n",
                        "epoch:  1211\n",
                        "epoch:  1212\n",
                        "epoch:  1213\n",
                        "step: 3640, loss: 148.67230224609375, data time: 4.015981674194336\n",
                        "epoch:  1214\n",
                        "epoch:  1215\n",
                        "epoch:  1216\n",
                        "step: 3650, loss: 183.22882080078125, data time: 1.6687026023864746\n",
                        "epoch:  1217\n",
                        "epoch:  1218\n",
                        "epoch:  1219\n",
                        "step: 3660, loss: 148.14114379882812, data time: 1.2044289112091064\n",
                        "epoch:  1220\n",
                        "epoch:  1221\n",
                        "epoch:  1222\n",
                        "epoch:  1223\n",
                        "step: 3670, loss: 138.45448303222656, data time: 4.406707763671875\n",
                        "epoch:  1224\n",
                        "epoch:  1225\n",
                        "epoch:  1226\n",
                        "step: 3680, loss: 191.20831298828125, data time: 2.0992289781570435\n",
                        "epoch:  1227\n",
                        "epoch:  1228\n",
                        "epoch:  1229\n",
                        "step: 3690, loss: 180.63314819335938, data time: 1.321155071258545\n",
                        "epoch:  1230\n",
                        "epoch:  1231\n",
                        "epoch:  1232\n",
                        "epoch:  1233\n",
                        "step: 3700, loss: 152.98419189453125, data time: 4.240328550338745\n",
                        "epoch:  1234\n",
                        "epoch:  1235\n",
                        "epoch:  1236\n",
                        "step: 3710, loss: 127.13148498535156, data time: 1.8464195728302002\n",
                        "epoch:  1237\n",
                        "epoch:  1238\n",
                        "epoch:  1239\n",
                        "step: 3720, loss: 160.2724609375, data time: 1.4637827078501384\n",
                        "epoch:  1240\n",
                        "epoch:  1241\n",
                        "epoch:  1242\n",
                        "epoch:  1243\n",
                        "step: 3730, loss: 183.67764282226562, data time: 4.07054877281189\n",
                        "epoch:  1244\n",
                        "epoch:  1245\n",
                        "epoch:  1246\n",
                        "step: 3740, loss: 160.28268432617188, data time: 1.8949055671691895\n",
                        "epoch:  1247\n",
                        "epoch:  1248\n",
                        "epoch:  1249\n",
                        "step: 3750, loss: 239.3914794921875, data time: 1.348804235458374\n",
                        "epoch:  1250\n",
                        "epoch:  1251\n",
                        "epoch:  1252\n",
                        "epoch:  1253\n",
                        "step: 3760, loss: 192.0331268310547, data time: 3.4568216800689697\n",
                        "epoch:  1254\n",
                        "epoch:  1255\n",
                        "epoch:  1256\n",
                        "step: 3770, loss: 217.01065063476562, data time: 1.959542155265808\n",
                        "epoch:  1257\n",
                        "epoch:  1258\n",
                        "epoch:  1259\n",
                        "step: 3780, loss: 181.123779296875, data time: 1.237143596013387\n",
                        "epoch:  1260\n",
                        "epoch:  1261\n",
                        "epoch:  1262\n",
                        "epoch:  1263\n",
                        "step: 3790, loss: 130.7170867919922, data time: 3.81774640083313\n",
                        "epoch:  1264\n",
                        "epoch:  1265\n",
                        "epoch:  1266\n",
                        "step: 3800, loss: 162.9492645263672, data time: 2.0320669412612915\n",
                        "epoch:  1267\n",
                        "epoch:  1268\n",
                        "epoch:  1269\n",
                        "step: 3810, loss: 135.304443359375, data time: 1.3477957248687744\n",
                        "epoch:  1270\n",
                        "epoch:  1271\n",
                        "epoch:  1272\n",
                        "epoch:  1273\n",
                        "step: 3820, loss: 150.2365264892578, data time: 3.6078991889953613\n",
                        "epoch:  1274\n",
                        "epoch:  1275\n",
                        "epoch:  1276\n",
                        "step: 3830, loss: 201.7290496826172, data time: 1.9584541320800781\n",
                        "epoch:  1277\n",
                        "epoch:  1278\n",
                        "epoch:  1279\n",
                        "step: 3840, loss: 233.67359924316406, data time: 1.4400989214579265\n",
                        "epoch:  1280\n",
                        "epoch:  1281\n",
                        "epoch:  1282\n",
                        "epoch:  1283\n",
                        "step: 3850, loss: 129.19891357421875, data time: 3.869234085083008\n",
                        "epoch:  1284\n",
                        "epoch:  1285\n",
                        "epoch:  1286\n",
                        "step: 3860, loss: 150.94712829589844, data time: 1.9532458782196045\n",
                        "epoch:  1287\n",
                        "epoch:  1288\n",
                        "epoch:  1289\n",
                        "step: 3870, loss: 202.4119415283203, data time: 1.3043962319691975\n",
                        "epoch:  1290\n",
                        "epoch:  1291\n",
                        "epoch:  1292\n",
                        "epoch:  1293\n",
                        "step: 3880, loss: 208.194091796875, data time: 4.18691349029541\n",
                        "epoch:  1294\n",
                        "epoch:  1295\n",
                        "epoch:  1296\n",
                        "step: 3890, loss: 154.51889038085938, data time: 1.960234522819519\n",
                        "epoch:  1297\n",
                        "epoch:  1298\n",
                        "epoch:  1299\n",
                        "step: 3900, loss: 146.06051635742188, data time: 1.4090221722920735\n",
                        "epoch:  1300\n",
                        "epoch:  1301\n",
                        "epoch:  1302\n",
                        "epoch:  1303\n",
                        "step: 3910, loss: 119.31904602050781, data time: 4.13493275642395\n",
                        "epoch:  1304\n",
                        "epoch:  1305\n",
                        "epoch:  1306\n",
                        "step: 3920, loss: 190.28131103515625, data time: 2.1033592224121094\n",
                        "epoch:  1307\n",
                        "epoch:  1308\n",
                        "epoch:  1309\n",
                        "step: 3930, loss: 125.60890197753906, data time: 1.3940455913543701\n",
                        "epoch:  1310\n",
                        "epoch:  1311\n",
                        "epoch:  1312\n",
                        "epoch:  1313\n",
                        "step: 3940, loss: 160.39242553710938, data time: 3.7308382987976074\n",
                        "epoch:  1314\n",
                        "epoch:  1315\n",
                        "epoch:  1316\n",
                        "step: 3950, loss: 223.91921997070312, data time: 1.9295905828475952\n",
                        "epoch:  1317\n",
                        "epoch:  1318\n",
                        "epoch:  1319\n",
                        "step: 3960, loss: 192.9283447265625, data time: 1.3806150754292805\n",
                        "epoch:  1320\n",
                        "epoch:  1321\n",
                        "epoch:  1322\n",
                        "epoch:  1323\n",
                        "step: 3970, loss: 126.24396514892578, data time: 3.938741445541382\n",
                        "epoch:  1324\n",
                        "epoch:  1325\n",
                        "epoch:  1326\n",
                        "step: 3980, loss: 129.86338806152344, data time: 2.1047779321670532\n",
                        "epoch:  1327\n",
                        "epoch:  1328\n",
                        "epoch:  1329\n",
                        "step: 3990, loss: 188.35287475585938, data time: 1.2323246002197266\n",
                        "epoch:  1330\n",
                        "epoch:  1331\n",
                        "epoch:  1332\n",
                        "epoch:  1333\n",
                        "step: 4000, loss: 110.29840850830078, data time: 3.8465757369995117\n",
                        "epoch:  1334\n",
                        "epoch:  1335\n",
                        "epoch:  1336\n",
                        "step: 4010, loss: 120.38029479980469, data time: 1.8814271688461304\n",
                        "epoch:  1337\n",
                        "epoch:  1338\n",
                        "epoch:  1339\n",
                        "step: 4020, loss: 142.4498291015625, data time: 1.2433679103851318\n",
                        "epoch:  1340\n",
                        "epoch:  1341\n",
                        "epoch:  1342\n",
                        "epoch:  1343\n",
                        "step: 4030, loss: 179.74172973632812, data time: 4.122354984283447\n",
                        "epoch:  1344\n",
                        "epoch:  1345\n",
                        "epoch:  1346\n",
                        "step: 4040, loss: 146.3334503173828, data time: 1.8833653926849365\n",
                        "epoch:  1347\n",
                        "epoch:  1348\n",
                        "epoch:  1349\n",
                        "step: 4050, loss: 103.72663879394531, data time: 1.214927037556966\n",
                        "epoch:  1350\n",
                        "epoch:  1351\n",
                        "epoch:  1352\n",
                        "epoch:  1353\n",
                        "step: 4060, loss: 147.57992553710938, data time: 3.969886064529419\n",
                        "epoch:  1354\n",
                        "epoch:  1355\n",
                        "epoch:  1356\n",
                        "step: 4070, loss: 197.6488800048828, data time: 1.7877103090286255\n",
                        "epoch:  1357\n",
                        "epoch:  1358\n",
                        "epoch:  1359\n",
                        "step: 4080, loss: 199.463623046875, data time: 1.3360904057820637\n",
                        "epoch:  1360\n",
                        "epoch:  1361\n",
                        "epoch:  1362\n",
                        "epoch:  1363\n",
                        "step: 4090, loss: 149.74546813964844, data time: 4.104359149932861\n",
                        "epoch:  1364\n",
                        "epoch:  1365\n",
                        "epoch:  1366\n",
                        "step: 4100, loss: 225.3626251220703, data time: 1.9932515621185303\n",
                        "epoch:  1367\n",
                        "epoch:  1368\n",
                        "epoch:  1369\n",
                        "step: 4110, loss: 176.63027954101562, data time: 1.326604684193929\n",
                        "epoch:  1370\n",
                        "epoch:  1371\n",
                        "epoch:  1372\n",
                        "epoch:  1373\n",
                        "step: 4120, loss: 137.29058837890625, data time: 4.250163316726685\n",
                        "epoch:  1374\n",
                        "epoch:  1375\n",
                        "epoch:  1376\n",
                        "step: 4130, loss: 162.0935821533203, data time: 2.0927820205688477\n",
                        "epoch:  1377\n",
                        "epoch:  1378\n",
                        "epoch:  1379\n",
                        "step: 4140, loss: 166.5054168701172, data time: 1.2508454322814941\n",
                        "epoch:  1380\n",
                        "epoch:  1381\n",
                        "epoch:  1382\n",
                        "epoch:  1383\n",
                        "step: 4150, loss: 140.38580322265625, data time: 3.9605295658111572\n",
                        "epoch:  1384\n",
                        "epoch:  1385\n",
                        "epoch:  1386\n",
                        "step: 4160, loss: 190.2904815673828, data time: 1.9830833673477173\n",
                        "epoch:  1387\n",
                        "epoch:  1388\n",
                        "epoch:  1389\n",
                        "step: 4170, loss: 130.29249572753906, data time: 1.2281098365783691\n",
                        "epoch:  1390\n",
                        "epoch:  1391\n",
                        "epoch:  1392\n",
                        "epoch:  1393\n",
                        "step: 4180, loss: 177.05740356445312, data time: 3.9930260181427\n",
                        "epoch:  1394\n",
                        "epoch:  1395\n",
                        "epoch:  1396\n",
                        "step: 4190, loss: 138.1936798095703, data time: 1.9600235223770142\n",
                        "epoch:  1397\n",
                        "epoch:  1398\n",
                        "epoch:  1399\n",
                        "step: 4200, loss: 130.11373901367188, data time: 1.2986512184143066\n",
                        "epoch:  1400\n",
                        "epoch:  1401\n",
                        "epoch:  1402\n",
                        "epoch:  1403\n",
                        "step: 4210, loss: 164.07049560546875, data time: 3.9016237258911133\n",
                        "epoch:  1404\n",
                        "epoch:  1405\n",
                        "epoch:  1406\n",
                        "step: 4220, loss: 186.40719604492188, data time: 1.973431944847107\n",
                        "epoch:  1407\n",
                        "epoch:  1408\n",
                        "epoch:  1409\n",
                        "step: 4230, loss: 155.10025024414062, data time: 1.3408552805582683\n",
                        "epoch:  1410\n",
                        "epoch:  1411\n",
                        "epoch:  1412\n",
                        "epoch:  1413\n",
                        "step: 4240, loss: 208.10354614257812, data time: 4.28394627571106\n",
                        "epoch:  1414\n",
                        "epoch:  1415\n",
                        "epoch:  1416\n",
                        "step: 4250, loss: 110.16502380371094, data time: 1.947740912437439\n",
                        "epoch:  1417\n",
                        "epoch:  1418\n",
                        "epoch:  1419\n",
                        "step: 4260, loss: 157.58311462402344, data time: 1.3479499816894531\n",
                        "epoch:  1420\n",
                        "epoch:  1421\n",
                        "epoch:  1422\n",
                        "epoch:  1423\n",
                        "step: 4270, loss: 184.9171905517578, data time: 3.8989226818084717\n",
                        "epoch:  1424\n",
                        "epoch:  1425\n",
                        "epoch:  1426\n",
                        "step: 4280, loss: 107.11585998535156, data time: 1.8460575342178345\n",
                        "epoch:  1427\n",
                        "epoch:  1428\n",
                        "epoch:  1429\n",
                        "step: 4290, loss: 141.4526824951172, data time: 1.2144596576690674\n",
                        "epoch:  1430\n",
                        "epoch:  1431\n",
                        "epoch:  1432\n",
                        "epoch:  1433\n",
                        "step: 4300, loss: 173.7718505859375, data time: 4.0832109451293945\n",
                        "epoch:  1434\n",
                        "epoch:  1435\n",
                        "epoch:  1436\n",
                        "step: 4310, loss: 182.14251708984375, data time: 2.137703537940979\n",
                        "epoch:  1437\n",
                        "epoch:  1438\n",
                        "epoch:  1439\n",
                        "step: 4320, loss: 244.6305694580078, data time: 1.3324787616729736\n",
                        "epoch:  1440\n",
                        "epoch:  1441\n",
                        "epoch:  1442\n",
                        "epoch:  1443\n",
                        "step: 4330, loss: 190.24874877929688, data time: 4.102244853973389\n",
                        "epoch:  1444\n",
                        "epoch:  1445\n",
                        "epoch:  1446\n",
                        "step: 4340, loss: 174.0448760986328, data time: 2.0149295330047607\n",
                        "epoch:  1447\n",
                        "epoch:  1448\n",
                        "epoch:  1449\n",
                        "step: 4350, loss: 127.30558013916016, data time: 1.3094396591186523\n",
                        "epoch:  1450\n",
                        "epoch:  1451\n",
                        "epoch:  1452\n",
                        "epoch:  1453\n",
                        "step: 4360, loss: 125.65580749511719, data time: 3.8725101947784424\n",
                        "epoch:  1454\n",
                        "epoch:  1455\n",
                        "epoch:  1456\n",
                        "step: 4370, loss: 120.48324584960938, data time: 1.8377145528793335\n",
                        "epoch:  1457\n",
                        "epoch:  1458\n",
                        "epoch:  1459\n",
                        "step: 4380, loss: 189.91226196289062, data time: 1.2553009986877441\n",
                        "epoch:  1460\n",
                        "epoch:  1461\n",
                        "epoch:  1462\n",
                        "epoch:  1463\n",
                        "step: 4390, loss: 125.9981689453125, data time: 3.8362245559692383\n",
                        "epoch:  1464\n",
                        "epoch:  1465\n",
                        "epoch:  1466\n",
                        "step: 4400, loss: 129.06690979003906, data time: 2.073335647583008\n",
                        "epoch:  1467\n",
                        "epoch:  1468\n",
                        "epoch:  1469\n",
                        "step: 4410, loss: 205.7921142578125, data time: 1.3979346752166748\n",
                        "epoch:  1470\n",
                        "epoch:  1471\n",
                        "epoch:  1472\n",
                        "epoch:  1473\n",
                        "step: 4420, loss: 216.36358642578125, data time: 4.242597341537476\n",
                        "epoch:  1474\n",
                        "epoch:  1475\n",
                        "epoch:  1476\n",
                        "step: 4430, loss: 108.48648071289062, data time: 2.0714128017425537\n",
                        "epoch:  1477\n",
                        "epoch:  1478\n",
                        "epoch:  1479\n",
                        "step: 4440, loss: 168.54994201660156, data time: 1.4017406304677327\n",
                        "epoch:  1480\n",
                        "epoch:  1481\n",
                        "epoch:  1482\n",
                        "epoch:  1483\n",
                        "step: 4450, loss: 119.1510009765625, data time: 3.995253562927246\n",
                        "epoch:  1484\n",
                        "epoch:  1485\n",
                        "epoch:  1486\n",
                        "step: 4460, loss: 142.03883361816406, data time: 1.8810694217681885\n",
                        "epoch:  1487\n",
                        "epoch:  1488\n",
                        "epoch:  1489\n",
                        "step: 4470, loss: 93.37962341308594, data time: 1.3380926450093586\n",
                        "epoch:  1490\n",
                        "epoch:  1491\n",
                        "epoch:  1492\n",
                        "epoch:  1493\n",
                        "step: 4480, loss: 141.14151000976562, data time: 4.087687015533447\n",
                        "epoch:  1494\n",
                        "epoch:  1495\n",
                        "epoch:  1496\n",
                        "step: 4490, loss: 175.16786193847656, data time: 2.1334331035614014\n",
                        "epoch:  1497\n",
                        "epoch:  1498\n",
                        "epoch:  1499\n",
                        "step: 4500, loss: 151.09323120117188, data time: 1.2969386577606201\n",
                        "train doned!\n"
                    ]
                }
            ],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "process = subprocess.Popen([\n",
                "    'python', 'train_diffusion.py',\n",
                "    '--config', 'mydataset.yml',\n",
                "    '--resume', 'WeatherDiff64.pth.tar'\n",
                "], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "\n",
                "try:\n",
                "    for line in process.stdout:\n",
                "        print(line, end='')\n",
                "        sys.stdout.flush()\n",
                "    process.wait()\n",
                "    if process.returncode != 0:\n",
                "        print(f\"Process exited with code {process.returncode}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a9b14454",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overwriting datasets/__init__.py\n"
                    ]
                }
            ],
            "source": [
                "%%writefile datasets/__init__.py\n",
                "from .snow100k import Snow100K\n",
                "from .raindrop import RainDrop\n",
                "from .outdoorrain import OutdoorRain\n",
                "from .ohaze import OHaze\n",
                "from .custom_haze import CustomHaze\n",
                "from .allweather import AllWeather\n",
                "\n",
                "def get_dataset(args, config):\n",
                "    # L·∫•y t√™n dataset t·ª´ file config\n",
                "    dataset_name = config.data.dataset\n",
                "    \n",
                "    if dataset_name == 'Snow100K':\n",
                "        return Snow100K(args, config)\n",
                "    elif dataset_name == 'RainDrop':\n",
                "        return RainDrop(args, config)\n",
                "    elif dataset_name == 'OutdoorRain':\n",
                "        return OutdoorRain(args, config)\n",
                "    elif dataset_name == 'OHaze':\n",
                "        return OHaze(args, config)\n",
                "    elif dataset_name == 'AllWeather':\n",
                "        return AllWeather(args, config)\n",
                "    \n",
                "    # --- S·ª¨A ·ªû ƒê√ÇY ---\n",
                "    # Thay v√¨ so s√°nh == 'CustomHaze', ta d√πng startswith\n",
                "    # ƒê·ªÉ ch·∫•p nh·∫≠n c·∫£ 'CustomHaze', 'CustomHaze_case1', 'CustomHaze_case2'...\n",
                "    elif dataset_name.startswith('CustomHaze'):\n",
                "        return CustomHaze(args, config)\n",
                "    \n",
                "    else:\n",
                "        raise KeyError(f\"Dataset kh√¥ng h·ª£p l·ªá: {dataset_name}. H√£y ki·ªÉm tra l·∫°i configs.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "203f1885",
            "metadata": {},
            "outputs": [],
            "source": [
                "# X√≥a c√°c file config c≈© ƒë·ªÉ tr√°nh conflict\n",
                "import os\n",
                "import glob\n",
                "\n",
                "old_configs = glob.glob(\"configs/case*.yml\")\n",
                "for f in old_configs:\n",
                "    try:\n",
                "        os.remove(f)\n",
                "        print(f\"üóëÔ∏è ƒê√£ x√≥a file config c≈©: {f}\")\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "print(\"‚úÖ S·∫µn s√†ng t·∫°o config m·ªõi!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "ace04f38",
            "metadata": {
                "collapsed": false,
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y './ckpts/WeatherDiff64.pth.tar'.\n",
                        "   Code s·∫Ω c·ªë g·∫Øng t√¨m tr√™n Volume ho·∫∑c train t·ª´ ƒë·∫ßu (Random Init).\n",
                        "   ‚úÖ ƒê√£ t·∫£i Base Model t·ª´ Volume v·ªÅ.\n",
                        "\n",
                        "============================================================\n",
                        "üöÄ RUNNING EXPERIMENT: case1_baseline\n",
                        "üìù Config Goc (No changes)\n",
                        "============================================================\n",
                        "‚ñ∂Ô∏è Executing: python train_diffusion.py --config case1_baseline.yml --sampling_timesteps 25 --image_folder results/case1_baseline_patches/\n",
                        "KeyError: 'CustomHaze_case1_baseline'\n",
                        "\n",
                        "üíæ ƒêang l∆∞u model case1_baseline l√™n Volume...\n",
                        "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file model output ƒë·ªÉ l∆∞u.\n",
                        "\n",
                        "============================================================\n",
                        "üöÄ RUNNING EXPERIMENT: case2_highWD\n",
                        "üìù Tang Weight Decay -> 0.01\n",
                        "============================================================\n",
                        "‚ñ∂Ô∏è Executing: python train_diffusion.py --config case2_highWD.yml --sampling_timesteps 25 --image_folder results/case2_highWD_patches/\n",
                        "KeyError: 'CustomHaze_case2_highWD'\n",
                        "\n",
                        "üíæ ƒêang l∆∞u model case2_highWD l√™n Volume...\n",
                        "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file model output ƒë·ªÉ l∆∞u.\n",
                        "\n",
                        "============================================================\n",
                        "üöÄ RUNNING EXPERIMENT: case3_lowLR\n",
                        "üìù Giam LR -> 5e-6\n",
                        "============================================================\n",
                        "‚ñ∂Ô∏è Executing: python train_diffusion.py --config case3_lowLR.yml --sampling_timesteps 25 --image_folder results/case3_lowLR_patches/\n",
                        "KeyError: 'CustomHaze_case3_lowLR'\n",
                        "\n",
                        "üõë D·ª´ng b·ªüi ng∆∞·ªùi d√πng.\n",
                        "\n",
                        "üéâ HO√ÄN T·∫§T TO√ÄN B·ªò 5 TH√ç NGHI·ªÜM!\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import yaml\n",
                "import subprocess\n",
                "import sys\n",
                "import modal\n",
                "import shutil\n",
                "\n",
                "# --- C·∫§U H√åNH ---\n",
                "BASE_CONFIG_PATH = \"configs/custom_haze.yml\"\n",
                "VOL_NAME = \"weather-diffusion-vol\"\n",
                "\n",
                "# T√™n file model g·ªëc b·∫°n mu·ªën d√πng ƒë·ªÉ finetune (C·∫¶N C√ì S·∫¥N trong folder ./ckpts)\n",
                "# N·∫øu b·∫°n ch∆∞a c√≥, h√£y ƒë·∫£m b·∫£o ƒë√£ upload file WeatherDiff64.pth.tar v√†o ./ckpts \n",
                "# ho·∫∑c code s·∫Ω t·ª± t√¨m trong Modal Volume\n",
                "BASE_MODEL_NAME = \"WeatherDiff64.pth.tar\" \n",
                "\n",
                "# ƒê·ªãnh nghƒ©a 5 th√≠ nghi·ªám\n",
                "experiments = {\n",
                "    \"case1_baseline\": {\n",
                "        \"desc\": \"Config Goc (No changes)\",\n",
                "        \"changes\": {} \n",
                "    },\n",
                "    \"case2_highWD\": {\n",
                "        \"desc\": \"Tang Weight Decay -> 0.01\",\n",
                "        \"changes\": {\"optim\": {\"weight_decay\": 0.01}}\n",
                "    },\n",
                "    \"case3_lowLR\": {\n",
                "        \"desc\": \"Giam LR -> 5e-6\",\n",
                "        \"changes\": {\"optim\": {\"lr\": 0.000005}}\n",
                "    },\n",
                "    \"case4_dropout\": {\n",
                "        \"desc\": \"Tang Dropout -> 0.2\",\n",
                "        \"changes\": {\"model\": {\"dropout\": 0.2}}\n",
                "    },\n",
                "    \"case5_combined\": {\n",
                "        \"desc\": \"Ket hop tat ca: WD=0.01, LR=5e-6, Dropout=0.2\",\n",
                "        \"changes\": {\n",
                "            \"optim\": {\"weight_decay\": 0.01, \"lr\": 0.000005},\n",
                "            \"model\": {\"dropout\": 0.2}\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "# H√†m helper update config\n",
                "def update_nested_dict(d, u):\n",
                "    for k, v in u.items():\n",
                "        if isinstance(v, dict):\n",
                "            d[k] = update_nested_dict(d.get(k, {}), v)\n",
                "        else:\n",
                "            d[k] = v\n",
                "    return d\n",
                "\n",
                "# K·∫øt n·ªëi Volume\n",
                "vol = modal.Volume.from_name(VOL_NAME, create_if_missing=True)\n",
                "\n",
                "# Ki·ªÉm tra base model local\n",
                "if not os.path.exists(f\"./ckpts/{BASE_MODEL_NAME}\"):\n",
                "    print(f\"‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y './ckpts/{BASE_MODEL_NAME}'.\")\n",
                "    print(\"   Code s·∫Ω c·ªë g·∫Øng t√¨m tr√™n Volume ho·∫∑c train t·ª´ ƒë·∫ßu (Random Init).\")\n",
                "    try:\n",
                "        vol.read_file(f\"checkpoints/{BASE_MODEL_NAME}\", f\"./ckpts/{BASE_MODEL_NAME}\")\n",
                "        print(\"   ‚úÖ ƒê√£ t·∫£i Base Model t·ª´ Volume v·ªÅ.\")\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "# --- V√íNG L·∫∂P CH√çNH ---\n",
                "for case_name, exp_data in experiments.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"üöÄ RUNNING EXPERIMENT: {case_name}\")\n",
                "    print(f\"üìù {exp_data['desc']}\")\n",
                "    print(f\"{'='*60}\")\n",
                "\n",
                "    # 1. T·∫°o Config\n",
                "    with open(BASE_CONFIG_PATH, 'r') as f:\n",
                "        cfg = yaml.safe_load(f)\n",
                "    \n",
                "    cfg = update_nested_dict(cfg, exp_data[\"changes\"])\n",
                "    # KH√îNG ƒë·ªïi dataset name, gi·ªØ nguy√™n \"CustomHaze\"\n",
                "    config_file = f\"configs/{case_name}.yml\"\n",
                "    with open(config_file, 'w') as f:\n",
                "        yaml.dump(cfg, f)\n",
                "    \n",
                "    # 2. Chu·∫©n b·ªã file Resume/Pretrained\n",
                "    # File checkpoint s·∫Ω c√≥ t√™n: CustomHaze_ddpm.pth.tar (m·∫∑c ƒë·ªãnh)\n",
                "    # ƒê·ªÉ tr√°nh ƒë√® l√™n nhau, ta s·∫Ω ƒë·ªïi t√™n sau khi train xong\n",
                "    default_ckpt_name = \"CustomHaze_ddpm.pth.tar\"\n",
                "    target_ckpt_name = f\"{case_name}_model.pth.tar\"\n",
                "    local_ckpt_path = f\"./ckpts/{default_ckpt_name}\"\n",
                "    final_ckpt_path = f\"./ckpts/{target_ckpt_name}\"\n",
                "    \n",
                "    resume_path = \"\"\n",
                "    \n",
                "    # Check xem case n√†y ƒë√£ t·ª´ng ch·∫°y d·ªü tr√™n Volume ch∆∞a?\n",
                "    try:\n",
                "        vol_files = [e.path for e in vol.listdir(f\"experiments/{case_name}\")]\n",
                "        if target_ckpt_name in [os.path.basename(p) for p in vol_files]:\n",
                "            print(\"üîÑ Ph√°t hi·ªán checkpoint c≈© tr√™n Volume. ƒêang t·∫£i v·ªÅ ƒë·ªÉ RESUME...\")\n",
                "            vol.read_file(f\"experiments/{case_name}/{target_ckpt_name}\", local_ckpt_path)\n",
                "            resume_path = local_ckpt_path\n",
                "    except:\n",
                "        pass \n",
                "\n",
                "    # N·∫øu ch∆∞a c√≥ file resume, d√πng Base Model ƒë·ªÉ b·∫Øt ƒë·∫ßu Finetune\n",
                "    if not resume_path and os.path.exists(f\"./ckpts/{BASE_MODEL_NAME}\"):\n",
                "        print(\"üÜï Ch∆∞a c√≥ checkpoint ri√™ng. Copy Base Model ƒë·ªÉ b·∫Øt ƒë·∫ßu FINETUNE...\")\n",
                "        shutil.copy(f\"./ckpts/{BASE_MODEL_NAME}\", local_ckpt_path)\n",
                "        resume_path = local_ckpt_path\n",
                "    \n",
                "    # 3. Ch·∫°y Training\n",
                "    cmd = [\n",
                "        'python', 'train_diffusion.py',\n",
                "        '--config', f'{case_name}.yml',\n",
                "        '--sampling_timesteps', '25',\n",
                "        '--image_folder', f'results/{case_name}_patches/'\n",
                "    ]\n",
                "    if resume_path:\n",
                "        cmd.extend(['--resume', resume_path])\n",
                "\n",
                "    print(f\"‚ñ∂Ô∏è Executing: {' '.join(cmd)}\")\n",
                "    \n",
                "    try:\n",
                "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
                "        for line in process.stdout:\n",
                "            # Ch·ªâ in c√°c d√≤ng quan tr·ªçng ƒë·ªÉ log ƒë·ª° d√†i\n",
                "            if any(k in line for k in [\"epoch:\", \"step:\", \"Error\", \"Resume\", \"loaded checkpoint\"]):\n",
                "                print(line.strip())\n",
                "        process.wait()\n",
                "        \n",
                "        if process.returncode != 0:\n",
                "            print(f\"‚ùå Training failed with code {process.returncode}\")\n",
                "            continue\n",
                "            \n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nüõë D·ª´ng b·ªüi ng∆∞·ªùi d√πng.\")\n",
                "        break # D·ª´ng to√†n b·ªô n·∫øu user b·∫•m stop\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå L·ªói: {e}\")\n",
                "        continue\n",
                "\n",
                "\n",
                "    # 4. ƒê·ªïi t√™n checkpoint ƒë·ªÉ tr√°nh ƒë√® l√™n nhau gi·ªØa c√°c experiment\n",
                "    if os.path.exists(local_ckpt_path):\n",
                "        shutil.move(local_ckpt_path, final_ckpt_path)\n",
                "        print(f\"üìù ƒê√£ ƒë·ªïi t√™n checkpoint: {default_ckpt_name} -> {target_ckpt_name}\")\n",
                "    \n",
                "    # 5. L∆∞u k·∫øt qu·∫£ l√™n Volume\n",
                "    print(f\"\\nüíæ ƒêang l∆∞u model {case_name} l√™n Volume...\")\n",
                "    if os.path.exists(final_ckpt_path):\n",
                "        remote_path = f\"experiments/{case_name}/{target_ckpt_name}\"\n",
                "        with vol.batch_upload() as batch:\n",
                "            batch.put_file(final_ckpt_path, remote_path)\n",
                "        print(f\"‚úÖ ƒê√£ l∆∞u: {remote_path}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file model output ƒë·ªÉ l∆∞u.\")\n",
                "\n",
                "print(\"\\nüéâ HO√ÄN T·∫§T TO√ÄN B·ªò 5 TH√ç NGHI·ªÜM!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
